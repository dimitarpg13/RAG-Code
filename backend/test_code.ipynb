{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2760a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.indexing.github_parsing import GitHubParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ee2d79d-772b-4b64-b4af-58d14d71ebe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/huggingface/transformers\"\n",
    "parser = GitHubParser(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aceb315a-d7eb-424e-8d1e-4e2f554322c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('huggingface', 'transformers', None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19476588-99e7-4f80-81f1-01dcd8428cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'huggingface'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(parser.owner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdf2cf24-3ceb-431f-ae2f-681940a3df86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'transformers'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(parser.repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9198fbf8-657f-42b6-93a3-f0d14a7e1002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(parser.ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7f3a4cd-41f9-4e44-b584-35111e5e40aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://codeload.github.com/huggingface/transformers/zip/master\n",
      "Response status code: 404\n",
      "URL: https://codeload.github.com/huggingface/transformers/zip/main\n",
      "Response status code: 200\n"
     ]
    }
   ],
   "source": [
    "zipb = parser.fetch_repo_zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8243fe4-6137-4b37-b638-37df5aa1eba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://codeload.github.com/huggingface/transformers/zip/master\n",
      "Response status code: 404\n",
      "URL: https://codeload.github.com/huggingface/transformers/zip/main\n",
      "Response status code: 200\n"
     ]
    }
   ],
   "source": [
    "from app.indexing.github_parsing import GitHubParser\n",
    "url = \"https://github.com/huggingface/transformers\"\n",
    "parser = GitHubParser(url)\n",
    "\n",
    "elements = parser.parse_repo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d597f2ae-c499-4a77-b8b1-0ea6f924d17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CodeElement(text='<!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\n\\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Efficient Training on Multiple GPUs\\n\\nå˜ä¸€ã®GPUã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒé…ã™ãã‚‹å ´åˆã‚„ã€ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ãŒå˜ä¸€ã®GPUã®ãƒ¡ãƒ¢ãƒªã«åã¾ã‚‰ãªã„å ´åˆã€è¤‡æ•°ã®GPUã‚’ä½¿ç”¨ã—ãŸã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå¿…è¦ã¨ãªã‚Šã¾ã™ã€‚å˜ä¸€ã®GPUã‹ã‚‰è¤‡æ•°ã®GPUã¸ã®åˆ‡ã‚Šæ›¿ãˆã«ã¯ã€ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã‚’åˆ†æ•£ã™ã‚‹ãŸã‚ã®ã‚ã‚‹ç¨®ã®ä¸¦åˆ—å‡¦ç†ãŒå¿…è¦ã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã€ãƒ†ãƒ³ã‚½ãƒ«ã€ã¾ãŸã¯ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä¸¦åˆ—å‡¦ç†ãªã©ã€ã•ã¾ã–ã¾ãªä¸¦åˆ—å‡¦ç†æŠ€è¡“ãŒã‚ã‚Šã¾ã™ã€‚ãŸã ã—ã€ã™ã¹ã¦ã«é©ã—ãŸä¸€ã¤ã®è§£æ±ºç­–ã¯å­˜åœ¨ã›ãšã€æœ€é©ãªè¨­å®šã¯ä½¿ç”¨ã™ã‚‹ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã«ä¾å­˜ã—ã¾ã™ã€‚ã“ã®è¨˜äº‹ã¯ã€ãŠãã‚‰ãä»–ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«ã‚‚é©ç”¨ã•ã‚Œã‚‹ä¸»è¦ãªæ¦‚å¿µã«ç„¦ç‚¹ã‚’å½“ã¦ã¤ã¤ã€PyTorchãƒ™ãƒ¼ã‚¹ã®å®Ÿè£…ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚\\n\\n<Tip>\\n\\n**æ³¨æ„**: [å˜ä¸€GPUã‚»ã‚¯ã‚·ãƒ§ãƒ³](perf_train_gpu_one) ã§ç´¹ä»‹ã•ã‚ŒãŸå¤šãã®æˆ¦ç•¥ï¼ˆæ··åˆç²¾åº¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚„å‹¾é…è“„ç©ãªã©ï¼‰ã¯ä¸€èˆ¬çš„ã§ã‚ã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä¸€èˆ¬çš„ã«é©ç”¨ã•ã‚Œã¾ã™ã€‚ã—ãŸãŒã£ã¦ã€ãƒãƒ«ãƒGPUã‚„CPUãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãªã©ã®æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«å…¥ã‚‹å‰ã«ã€ãã‚Œã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\\n\\n</Tip>\\n\\nã¾ãšã€ã•ã¾ã–ã¾ãª1Dä¸¦åˆ—å‡¦ç†æŠ€è¡“ã¨ãã®åˆ©ç‚¹ãŠã‚ˆã³æ¬ ç‚¹ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã€ãã‚Œã‚‰ã‚’2DãŠã‚ˆã³3Dä¸¦åˆ—å‡¦ç†ã«çµ„ã¿åˆã‚ã›ã¦ã•ã‚‰ã«é«˜é€Ÿãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿç¾ã—ã€ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹æ–¹æ³•ã‚’æ¤œè¨ã—ã¾ã™ã€‚ã•ã¾ã–ã¾ãªä»–ã®å¼·åŠ›ãªä»£æ›¿æ‰‹æ³•ã‚‚ç´¹ä»‹ã•ã‚Œã¾ã™ã€‚\\n\\n## Concepts\\n\\nä»¥ä¸‹ã¯ã€ã“ã®æ–‡æ›¸ã§å¾Œã§è©³ã—ãèª¬æ˜ã•ã‚Œã‚‹ä¸»è¦ãªæ¦‚å¿µã®ç°¡å˜ãªèª¬æ˜ã§ã™ã€‚\\n\\n1. **DataParallel (DP)** - åŒã˜ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒè¤‡æ•°å›è¤‡è£½ã•ã‚Œã€å„ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ãƒ©ã‚¤ã‚¹ãŒä¾›çµ¦ã•ã‚Œã¾ã™ã€‚å‡¦ç†ã¯ä¸¦è¡Œã—ã¦è¡Œã‚ã‚Œã€å„ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—ã®æœ€å¾Œã«åŒæœŸã•ã‚Œã¾ã™ã€‚\\n2. **TensorParallel (TP)** - å„ãƒ†ãƒ³ã‚½ãƒ«ã¯è¤‡æ•°ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã•ã‚Œã€å˜ä¸€ã®GPUã«ãƒ†ãƒ³ã‚½ãƒ«å…¨ä½“ãŒå­˜åœ¨ã™ã‚‹ã®ã§ã¯ãªãã€ãƒ†ãƒ³ã‚½ãƒ«ã®å„ã‚·ãƒ£ãƒ¼ãƒ‰ãŒæŒ‡å®šã•ã‚ŒãŸGPUã«å­˜åœ¨ã—ã¾ã™ã€‚å‡¦ç†ä¸­ã«ã€å„ã‚·ãƒ£ãƒ¼ãƒ‰ã¯åˆ¥ã€…ã«ä¸¦è¡Œã—ã¦å‡¦ç†ã•ã‚Œã€ç•°ãªã‚‹GPUã§åŒæœŸã•ã‚Œã€ã‚¹ãƒ†ãƒƒãƒ—ã®æœ€å¾Œã«çµæœãŒåŒæœŸã•ã‚Œã¾ã™ã€‚ã“ã‚Œã¯æ°´å¹³ä¸¦åˆ—å‡¦ç†ã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã§ã€åˆ†å‰²ã¯æ°´å¹³ãƒ¬ãƒ™ãƒ«ã§è¡Œã‚ã‚Œã¾ã™ã€‚\\n3. **PipelineParallel (PP)** - ãƒ¢ãƒ‡ãƒ«ã¯å‚ç›´ï¼ˆãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ¬ãƒ™ãƒ«ï¼‰ã«è¤‡æ•°ã®GPUã«åˆ†å‰²ã•ã‚Œã€ãƒ¢ãƒ‡ãƒ«ã®å˜ä¸€ã¾ãŸã¯è¤‡æ•°ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒå˜ä¸€ã®GPUã«é…ç½®ã•ã‚Œã¾ã™ã€‚å„GPUã¯ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ç•°ãªã‚‹ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’ä¸¦è¡Œã—ã¦å‡¦ç†ã—ã€ãƒãƒƒãƒã®å°ã•ãªãƒãƒ£ãƒ³ã‚¯ã§ä½œæ¥­ã—ã¾ã™ã€‚\\n4. **Zero Redundancy Optimizer (ZeRO)** - TPã¨ã„ãã‚‰ã‹ä¼¼ãŸã‚ˆã†ãªãƒ†ãƒ³ã‚½ãƒ«ã®ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™ãŒã€å‰å‘ãã¾ãŸã¯å¾Œå‘ãã®è¨ˆç®—ã®ãŸã‚ã«ãƒ†ãƒ³ã‚½ãƒ«å…¨ä½“ãŒå†æ§‹ç¯‰ã•ã‚Œã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›´ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã¾ãŸã€GPUãƒ¡ãƒ¢ãƒªãŒåˆ¶é™ã•ã‚Œã¦ã„ã‚‹å ´åˆã«è£œå„Ÿã™ã‚‹ãŸã‚ã®ã•ã¾ã–ã¾ãªã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰æŠ€è¡“ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚\\n5. **Sharded DDP** - Sharded DDPã¯ã€ã•ã¾ã–ã¾ãªZeROå®Ÿè£…ã§ä½¿ç”¨ã•ã‚Œã‚‹åŸºæœ¬çš„ãªZeROã‚³ãƒ³ã‚»ãƒ—ãƒˆã®åˆ¥åã§ã™ã€‚\\n\\nå„ã‚³ãƒ³ã‚»ãƒ—ãƒˆã®è©³ç´°ã«æ·±å…¥ã‚Šã™ã‚‹å‰ã«ã€å¤§è¦æ¨¡ãªã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ã§å¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹éš›ã®å¤§ã¾ã‹ãªæ±ºå®šãƒ—ãƒ­ã‚»ã‚¹ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\\n\\n## Scalability Strategy\\n\\n**â‡¨ ã‚·ãƒ³ã‚°ãƒ«ãƒãƒ¼ãƒ‰ / ãƒãƒ«ãƒGPU**\\n* ãƒ¢ãƒ‡ãƒ«ãŒå˜ä¸€ã®GPUã«åã¾ã‚‹å ´åˆï¼š\\n\\n    1. DDP - åˆ†æ•£ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—\\n    2. ZeRO - çŠ¶æ³ã¨ä½¿ç”¨ã•ã‚Œã‚‹æ§‹æˆã«å¿œã˜ã¦é€Ÿã„ã‹ã©ã†ã‹ãŒç•°ãªã‚Šã¾ã™\\n\\n* ãƒ¢ãƒ‡ãƒ«ãŒå˜ä¸€ã®GPUã«åã¾ã‚‰ãªã„å ´åˆï¼š\\n\\n    1. PP\\n    2. ZeRO\\n    3. TP\\n\\n    éå¸¸ã«é«˜é€Ÿãªãƒãƒ¼ãƒ‰å†…æ¥ç¶šï¼ˆNVLINKã¾ãŸã¯NVSwitchãªã©ï¼‰ãŒã‚ã‚Œã°ã€ã“ã‚Œã‚‰ã®3ã¤ã¯ã»ã¼åŒã˜é€Ÿåº¦ã«ãªã‚‹ã¯ãšã§ã€ã“ã‚Œã‚‰ãŒãªã„å ´åˆã€PPã¯TPã¾ãŸã¯ZeROã‚ˆã‚Šã‚‚é€Ÿããªã‚Šã¾ã™ã€‚TPã®ç¨‹åº¦ã‚‚å·®ã‚’ç”Ÿã˜ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ç‰¹å®šã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã§ã®å‹è€…ã‚’è¦‹ã¤ã‘ã‚‹ãŸã‚ã«å®Ÿé¨“ã™ã‚‹ã“ã¨ãŒæœ€å–„ã§ã™ã€‚\\n\\n    TPã¯ã»ã¨ã‚“ã©ã®å ´åˆã€å˜ä¸€ãƒãƒ¼ãƒ‰å†…ã§ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚ã¤ã¾ã‚Šã€TPã‚µã‚¤ã‚º <= ãƒãƒ¼ãƒ‰ã”ã¨ã®GPUæ•°ã§ã™ã€‚\\n\\n* æœ€å¤§ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒå˜ä¸€ã®GPUã«åã¾ã‚‰ãªã„å ´åˆï¼š\\n\\n    1. ZeROã‚’ä½¿ç”¨ã—ãªã„å ´åˆ - TPã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚PPå˜ç‹¬ã§ã¯åã¾ã‚‰ãªã„ã§ã—ã‚‡ã†ã€‚\\n    2. ZeROã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ - \"ã‚·ãƒ³ã‚°ãƒ«GPU\"ã®ã‚¨ãƒ³ãƒˆãƒªã¨åŒã˜ã‚‚ã®ã‚’å‚ç…§ã—ã¦ãã ã•ã„\\n\\n**â‡¨ ãƒãƒ«ãƒãƒãƒ¼ãƒ‰ / ãƒãƒ«ãƒGPU**\\n\\n* ãƒãƒ¼ãƒ‰é–“ã®é«˜é€Ÿæ¥ç¶šãŒã‚ã‚‹å ´åˆï¼š\\n\\n    1. ZeRO - ãƒ¢ãƒ‡ãƒ«ã¸ã®ã»ã¨ã‚“ã©ã®å¤‰æ›´ãŒä¸è¦ã§ã™\\n    2. PP+TP+DP - é€šä¿¡ãŒå°‘ãªãã€ãƒ¢ãƒ‡ãƒ«ã¸ã®å¤§è¦æ¨¡ãªå¤‰æ›´ãŒå¿…è¦ã§ã™\\n\\n* ãƒãƒ¼ãƒ‰é–“ã®æ¥ç¶šãŒé…ãã€GPUãƒ¡ãƒ¢ãƒªãŒã¾ã ä¸è¶³ã—ã¦ã„ã‚‹å ´åˆï¼š\\n\\n    1. DP+PP+TP+ZeRO-1\\n\\n## Data Parallelism\\n\\n2ã¤ã®GPUã‚’æŒã¤ã»ã¨ã‚“ã©ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã€`DataParallel`ï¼ˆDPï¼‰ã¨`DistributedDataParallel`ï¼ˆDDPï¼‰ã«ã‚ˆã£ã¦æä¾›ã•ã‚Œã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€Ÿåº¦ã®å‘ä¸Šã‚’ã™ã§ã«äº«å—ã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚‰ã¯ã»ã¼è‡ªæ˜ã«ä½¿ç”¨ã§ãã‚‹PyTorchã®çµ„ã¿è¾¼ã¿æ©Ÿèƒ½ã§ã™ã€‚ä¸€èˆ¬çš„ã«ã€ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã§å‹•ä½œã™ã‚‹DDPã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚DPã¯ä¸€éƒ¨ã®ãƒ¢ãƒ‡ãƒ«ã§å¤±æ•—ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã§ã™ã€‚[PyTorchã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³](https://pytorch.org/docs/master/generated/torch.nn.DataParallel.html)è‡ªä½“ã‚‚DDPã®ä½¿ç”¨ã‚’æ¨å¥¨ã—ã¦ã„ã¾ã™ã€‚\\n\\n### DP vs DDP\\n\\n`DistributedDataParallel`ï¼ˆDDPï¼‰ã¯é€šå¸¸ã€`DataParallel`ï¼ˆDPï¼‰ã‚ˆã‚Šã‚‚é«˜é€Ÿã§ã™ãŒã€å¸¸ã«ãã†ã¨ã¯é™ã‚Šã¾ã›ã‚“ï¼š\\n* DPã¯Pythonã‚¹ãƒ¬ãƒƒãƒ‰ãƒ™ãƒ¼ã‚¹ã§ã™ãŒã€DDPã¯ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ãƒ™ãƒ¼ã‚¹ã§ã™ã€‚ãã®ãŸã‚ã€GILï¼ˆGlobal Interpreter Lockï¼‰ãªã©ã®Pythonã‚¹ãƒ¬ãƒƒãƒ‰ã®åˆ¶ç´„ãŒãªã„ãŸã‚ã§ã™ã€‚\\n* ä¸€æ–¹ã€GPUã‚«ãƒ¼ãƒ‰é–“ã®é…ã„ç›¸äº’æ¥ç¶šæ€§ã¯ã€DDPã®å ´åˆã«å®Ÿéš›ã«ã¯é…ã„çµæœã‚’ã‚‚ãŸã‚‰ã™å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\\n\\nä»¥ä¸‹ã¯ã€2ã¤ã®ãƒ¢ãƒ¼ãƒ‰é–“ã®GPUé–“é€šä¿¡ã®ä¸»ãªé•ã„ã§ã™ï¼š\\n\\n[DDP](https://pytorch.org/docs/master/notes/ddp.html):\\n\\n- é–‹å§‹æ™‚ã€ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã¯ãƒ¢ãƒ‡ãƒ«ã‚’GPU 0ã‹ã‚‰ä»–ã®GPUã«è¤‡è£½ã—ã¾ã™ã€‚\\n- ãã‚Œã‹ã‚‰å„ãƒãƒƒãƒã”ã¨ã«:\\n   1. å„GPUã¯å„è‡ªã®ãƒŸãƒ‹ãƒãƒƒãƒã®ãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥æ¶ˆè²»ã—ã¾ã™ã€‚\\n   2. `backward`ä¸­ã€ãƒ­ãƒ¼ã‚«ãƒ«å‹¾é…ãŒæº–å‚™ã§ãã‚‹ã¨ã€ãã‚Œã‚‰ã¯ã™ã¹ã¦ã®ãƒ—ãƒ­ã‚»ã‚¹ã§å¹³å‡åŒ–ã•ã‚Œã¾ã™ã€‚\\n\\n[DP](https://pytorch.org/docs/master/generated/torch.nn.DataParallel.html):\\n\\nå„ãƒãƒƒãƒã”ã¨ã«:\\n   1. GPU 0ã¯ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒã‚’èª­ã¿å–ã‚Šã€ãã‚Œã‹ã‚‰å„GPUã«ãƒŸãƒ‹ãƒãƒƒãƒã‚’é€ä¿¡ã—ã¾ã™ã€‚\\n   2. GPU 0ã‹ã‚‰å„GPUã«æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¤‡è£½ã—ã¾ã™ã€‚\\n   3. `forward`ã‚’å®Ÿè¡Œã—ã€å„GPUã‹ã‚‰GPU 0ã«å‡ºåŠ›ã‚’é€ä¿¡ã—ã€æå¤±ã‚’è¨ˆç®—ã—ã¾ã™ã€‚\\n   4. GPU 0ã‹ã‚‰ã™ã¹ã¦ã®GPUã«æå¤±ã‚’åˆ†æ•£ã—ã€`backward`ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\\n   5. å„GPUã‹ã‚‰GPU 0ã«å‹¾é…ã‚’é€ä¿¡ã—ã€ãã‚Œã‚‰ã‚’å¹³å‡åŒ–ã—ã¾ã™ã€‚', source='docs/source/ja/perf_train_gpu_many.md', header='', extension='.md', description='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(elements[2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac1001da-5a7f-40d9-87c6-a13b96ab736a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "\n",
      "âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n",
      "rendered properly in your Markdown viewer.\n",
      "\n",
      "-->\n",
      "\n",
      "# Efficient Training on Multiple GPUs\n",
      "\n",
      "å˜ä¸€ã®GPUã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒé…ã™ãã‚‹å ´åˆã‚„ã€ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ãŒå˜ä¸€ã®GPUã®ãƒ¡ãƒ¢ãƒªã«åã¾ã‚‰ãªã„å ´åˆã€è¤‡æ•°ã®GPUã‚’ä½¿ç”¨ã—ãŸã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå¿…è¦ã¨ãªã‚Šã¾ã™ã€‚å˜ä¸€ã®GPUã‹ã‚‰è¤‡æ•°ã®GPUã¸ã®åˆ‡ã‚Šæ›¿ãˆã«ã¯ã€ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã‚’åˆ†æ•£ã™ã‚‹ãŸã‚ã®ã‚ã‚‹ç¨®ã®ä¸¦åˆ—å‡¦ç†ãŒå¿…è¦ã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã€ãƒ†ãƒ³ã‚½ãƒ«ã€ã¾ãŸã¯ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä¸¦åˆ—å‡¦ç†ãªã©ã€ã•ã¾ã–ã¾ãªä¸¦åˆ—å‡¦ç†æŠ€è¡“ãŒã‚ã‚Šã¾ã™ã€‚ãŸã ã—ã€ã™ã¹ã¦ã«é©ã—ãŸä¸€ã¤ã®è§£æ±ºç­–ã¯å­˜åœ¨ã›ãšã€æœ€é©ãªè¨­å®šã¯ä½¿ç”¨ã™ã‚‹ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã«ä¾å­˜ã—ã¾ã™ã€‚ã“ã®è¨˜äº‹ã¯ã€ãŠãã‚‰ãä»–ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«ã‚‚é©ç”¨ã•ã‚Œã‚‹ä¸»è¦ãªæ¦‚å¿µã«ç„¦ç‚¹ã‚’å½“ã¦ã¤ã¤ã€PyTorchãƒ™ãƒ¼ã‚¹ã®å®Ÿè£…ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚\n",
      "\n",
      "<Tip>\n",
      "\n",
      "**æ³¨æ„**: [å˜ä¸€GPUã‚»ã‚¯ã‚·ãƒ§ãƒ³](perf_train_gpu_one) ã§ç´¹ä»‹ã•ã‚ŒãŸå¤šãã®æˆ¦ç•¥ï¼ˆæ··åˆç²¾åº¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚„å‹¾é…è“„ç©ãªã©ï¼‰ã¯ä¸€èˆ¬çš„ã§ã‚ã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä¸€èˆ¬çš„ã«é©ç”¨ã•ã‚Œã¾ã™ã€‚ã—ãŸãŒã£ã¦ã€ãƒãƒ«ãƒGPUã‚„CPUãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãªã©ã®æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«å…¥ã‚‹å‰ã«ã€ãã‚Œã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n",
      "\n",
      "</Tip>\n",
      "\n",
      "ã¾ãšã€ã•ã¾ã–ã¾ãª1Dä¸¦åˆ—å‡¦ç†æŠ€è¡“ã¨ãã®åˆ©ç‚¹ãŠã‚ˆã³æ¬ ç‚¹ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã€ãã‚Œã‚‰ã‚’2DãŠã‚ˆã³3Dä¸¦åˆ—å‡¦ç†ã«çµ„ã¿åˆã‚ã›ã¦ã•ã‚‰ã«é«˜é€Ÿãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿç¾ã—ã€ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹æ–¹æ³•ã‚’æ¤œè¨ã—ã¾ã™ã€‚ã•ã¾ã–ã¾ãªä»–ã®å¼·åŠ›ãªä»£æ›¿æ‰‹æ³•ã‚‚ç´¹ä»‹ã•ã‚Œã¾ã™ã€‚\n",
      "\n",
      "## Concepts\n",
      "\n",
      "ä»¥ä¸‹ã¯ã€ã“ã®æ–‡æ›¸ã§å¾Œã§è©³ã—ãèª¬æ˜ã•ã‚Œã‚‹ä¸»è¦ãªæ¦‚å¿µã®ç°¡å˜ãªèª¬æ˜ã§ã™ã€‚\n",
      "\n",
      "1. **DataParallel (DP)** - åŒã˜ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒè¤‡æ•°å›è¤‡è£½ã•ã‚Œã€å„ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ãƒ©ã‚¤ã‚¹ãŒä¾›çµ¦ã•ã‚Œã¾ã™ã€‚å‡¦ç†ã¯ä¸¦è¡Œã—ã¦è¡Œã‚ã‚Œã€å„ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—ã®æœ€å¾Œã«åŒæœŸã•ã‚Œã¾ã™ã€‚\n",
      "2. **TensorParallel (TP)** - å„ãƒ†ãƒ³ã‚½ãƒ«ã¯è¤‡æ•°ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã•ã‚Œã€å˜ä¸€ã®GPUã«ãƒ†ãƒ³ã‚½ãƒ«å…¨ä½“ãŒå­˜åœ¨ã™ã‚‹ã®ã§ã¯ãªãã€ãƒ†ãƒ³ã‚½ãƒ«ã®å„ã‚·ãƒ£ãƒ¼ãƒ‰ãŒæŒ‡å®šã•ã‚ŒãŸGPUã«å­˜åœ¨ã—ã¾ã™ã€‚å‡¦ç†ä¸­ã«ã€å„ã‚·ãƒ£ãƒ¼ãƒ‰ã¯åˆ¥ã€…ã«ä¸¦è¡Œã—ã¦å‡¦ç†ã•ã‚Œã€ç•°ãªã‚‹GPUã§åŒæœŸã•ã‚Œã€ã‚¹ãƒ†ãƒƒãƒ—ã®æœ€å¾Œã«çµæœãŒåŒæœŸã•ã‚Œã¾ã™ã€‚ã“ã‚Œã¯æ°´å¹³ä¸¦åˆ—å‡¦ç†ã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã§ã€åˆ†å‰²ã¯æ°´å¹³ãƒ¬ãƒ™ãƒ«ã§è¡Œã‚ã‚Œã¾ã™ã€‚\n",
      "3. **PipelineParallel (PP)** - ãƒ¢ãƒ‡ãƒ«ã¯å‚ç›´ï¼ˆãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ¬ãƒ™ãƒ«ï¼‰ã«è¤‡æ•°ã®GPUã«åˆ†å‰²ã•ã‚Œã€ãƒ¢ãƒ‡ãƒ«ã®å˜ä¸€ã¾ãŸã¯è¤‡æ•°ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒå˜ä¸€ã®GPUã«é…ç½®ã•ã‚Œã¾ã™ã€‚å„GPUã¯ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ç•°ãªã‚‹ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’ä¸¦è¡Œã—ã¦å‡¦ç†ã—ã€ãƒãƒƒãƒã®å°ã•ãªãƒãƒ£ãƒ³ã‚¯ã§ä½œæ¥­ã—ã¾ã™ã€‚\n",
      "4. **Zero Redundancy Optimizer (ZeRO)** - TPã¨ã„ãã‚‰ã‹ä¼¼ãŸã‚ˆã†ãªãƒ†ãƒ³ã‚½ãƒ«ã®ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™ãŒã€å‰å‘ãã¾ãŸã¯å¾Œå‘ãã®è¨ˆç®—ã®ãŸã‚ã«ãƒ†ãƒ³ã‚½ãƒ«å…¨ä½“ãŒå†æ§‹ç¯‰ã•ã‚Œã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›´ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã¾ãŸã€GPUãƒ¡ãƒ¢ãƒªãŒåˆ¶é™ã•ã‚Œã¦ã„ã‚‹å ´åˆã«è£œå„Ÿã™ã‚‹ãŸã‚ã®ã•ã¾ã–ã¾ãªã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰æŠ€è¡“ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚\n",
      "5. **Sharded DDP** - Sharded DDPã¯ã€ã•ã¾ã–ã¾ãªZeROå®Ÿè£…ã§ä½¿ç”¨ã•ã‚Œã‚‹åŸºæœ¬çš„ãªZeROã‚³ãƒ³ã‚»ãƒ—ãƒˆã®åˆ¥åã§ã™ã€‚\n",
      "\n",
      "å„ã‚³ãƒ³ã‚»ãƒ—ãƒˆã®è©³ç´°ã«æ·±å…¥ã‚Šã™ã‚‹å‰ã«ã€å¤§è¦æ¨¡ãªã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ã§å¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹éš›ã®å¤§ã¾ã‹ãªæ±ºå®šãƒ—ãƒ­ã‚»ã‚¹ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
      "\n",
      "## Scalability Strategy\n",
      "\n",
      "**â‡¨ ã‚·ãƒ³ã‚°ãƒ«ãƒãƒ¼ãƒ‰ / ãƒãƒ«ãƒGPU**\n",
      "* ãƒ¢ãƒ‡ãƒ«ãŒå˜ä¸€ã®GPUã«åã¾ã‚‹å ´åˆï¼š\n",
      "\n",
      "    1. DDP - åˆ†æ•£ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—\n",
      "    2. ZeRO - çŠ¶æ³ã¨ä½¿ç”¨ã•ã‚Œã‚‹æ§‹æˆã«å¿œã˜ã¦é€Ÿã„ã‹ã©ã†ã‹ãŒç•°ãªã‚Šã¾ã™\n",
      "\n",
      "* ãƒ¢ãƒ‡ãƒ«ãŒå˜ä¸€ã®GPUã«åã¾ã‚‰ãªã„å ´åˆï¼š\n",
      "\n",
      "    1. PP\n",
      "    2. ZeRO\n",
      "    3. TP\n",
      "\n",
      "    éå¸¸ã«é«˜é€Ÿãªãƒãƒ¼ãƒ‰å†…æ¥ç¶šï¼ˆNVLINKã¾ãŸã¯NVSwitchãªã©ï¼‰ãŒã‚ã‚Œã°ã€ã“ã‚Œã‚‰ã®3ã¤ã¯ã»ã¼åŒã˜é€Ÿåº¦ã«ãªã‚‹ã¯ãšã§ã€ã“ã‚Œã‚‰ãŒãªã„å ´åˆã€PPã¯TPã¾ãŸã¯ZeROã‚ˆã‚Šã‚‚é€Ÿããªã‚Šã¾ã™ã€‚TPã®ç¨‹åº¦ã‚‚å·®ã‚’ç”Ÿã˜ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ç‰¹å®šã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã§ã®å‹è€…ã‚’è¦‹ã¤ã‘ã‚‹ãŸã‚ã«å®Ÿé¨“ã™ã‚‹ã“ã¨ãŒæœ€å–„ã§ã™ã€‚\n",
      "\n",
      "    TPã¯ã»ã¨ã‚“ã©ã®å ´åˆã€å˜ä¸€ãƒãƒ¼ãƒ‰å†…ã§ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚ã¤ã¾ã‚Šã€TPã‚µã‚¤ã‚º <= ãƒãƒ¼ãƒ‰ã”ã¨ã®GPUæ•°ã§ã™ã€‚\n",
      "\n",
      "* æœ€å¤§ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒå˜ä¸€ã®GPUã«åã¾ã‚‰ãªã„å ´åˆï¼š\n",
      "\n",
      "    1. ZeROã‚’ä½¿ç”¨ã—ãªã„å ´åˆ - TPã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚PPå˜ç‹¬ã§ã¯åã¾ã‚‰ãªã„ã§ã—ã‚‡ã†ã€‚\n",
      "    2. ZeROã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ - \"ã‚·ãƒ³ã‚°ãƒ«GPU\"ã®ã‚¨ãƒ³ãƒˆãƒªã¨åŒã˜ã‚‚ã®ã‚’å‚ç…§ã—ã¦ãã ã•ã„\n",
      "\n",
      "**â‡¨ ãƒãƒ«ãƒãƒãƒ¼ãƒ‰ / ãƒãƒ«ãƒGPU**\n",
      "\n",
      "* ãƒãƒ¼ãƒ‰é–“ã®é«˜é€Ÿæ¥ç¶šãŒã‚ã‚‹å ´åˆï¼š\n",
      "\n",
      "    1. ZeRO - ãƒ¢ãƒ‡ãƒ«ã¸ã®ã»ã¨ã‚“ã©ã®å¤‰æ›´ãŒä¸è¦ã§ã™\n",
      "    2. PP+TP+DP - é€šä¿¡ãŒå°‘ãªãã€ãƒ¢ãƒ‡ãƒ«ã¸ã®å¤§è¦æ¨¡ãªå¤‰æ›´ãŒå¿…è¦ã§ã™\n",
      "\n",
      "* ãƒãƒ¼ãƒ‰é–“ã®æ¥ç¶šãŒé…ãã€GPUãƒ¡ãƒ¢ãƒªãŒã¾ã ä¸è¶³ã—ã¦ã„ã‚‹å ´åˆï¼š\n",
      "\n",
      "    1. DP+PP+TP+ZeRO-1\n",
      "\n",
      "## Data Parallelism\n",
      "\n",
      "2ã¤ã®GPUã‚’æŒã¤ã»ã¨ã‚“ã©ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã€`DataParallel`ï¼ˆDPï¼‰ã¨`DistributedDataParallel`ï¼ˆDDPï¼‰ã«ã‚ˆã£ã¦æä¾›ã•ã‚Œã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€Ÿåº¦ã®å‘ä¸Šã‚’ã™ã§ã«äº«å—ã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚‰ã¯ã»ã¼è‡ªæ˜ã«ä½¿ç”¨ã§ãã‚‹PyTorchã®çµ„ã¿è¾¼ã¿æ©Ÿèƒ½ã§ã™ã€‚ä¸€èˆ¬çš„ã«ã€ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã§å‹•ä½œã™ã‚‹DDPã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚DPã¯ä¸€éƒ¨ã®ãƒ¢ãƒ‡ãƒ«ã§å¤±æ•—ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã§ã™ã€‚[PyTorchã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³](https://pytorch.org/docs/master/generated/torch.nn.DataParallel.html)è‡ªä½“ã‚‚DDPã®ä½¿ç”¨ã‚’æ¨å¥¨ã—ã¦ã„ã¾ã™ã€‚\n",
      "\n",
      "### DP vs DDP\n",
      "\n",
      "`DistributedDataParallel`ï¼ˆDDPï¼‰ã¯é€šå¸¸ã€`DataParallel`ï¼ˆDPï¼‰ã‚ˆã‚Šã‚‚é«˜é€Ÿã§ã™ãŒã€å¸¸ã«ãã†ã¨ã¯é™ã‚Šã¾ã›ã‚“ï¼š\n",
      "* DPã¯Pythonã‚¹ãƒ¬ãƒƒãƒ‰ãƒ™ãƒ¼ã‚¹ã§ã™ãŒã€DDPã¯ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ãƒ™ãƒ¼ã‚¹ã§ã™ã€‚ãã®ãŸã‚ã€GILï¼ˆGlobal Interpreter Lockï¼‰ãªã©ã®Pythonã‚¹ãƒ¬ãƒƒãƒ‰ã®åˆ¶ç´„ãŒãªã„ãŸã‚ã§ã™ã€‚\n",
      "* ä¸€æ–¹ã€GPUã‚«ãƒ¼ãƒ‰é–“ã®é…ã„ç›¸äº’æ¥ç¶šæ€§ã¯ã€DDPã®å ´åˆã«å®Ÿéš›ã«ã¯é…ã„çµæœã‚’ã‚‚ãŸã‚‰ã™å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n",
      "\n",
      "ä»¥ä¸‹ã¯ã€2ã¤ã®ãƒ¢ãƒ¼ãƒ‰é–“ã®GPUé–“é€šä¿¡ã®ä¸»ãªé•ã„ã§ã™ï¼š\n",
      "\n",
      "[DDP](https://pytorch.org/docs/master/notes/ddp.html):\n",
      "\n",
      "- é–‹å§‹æ™‚ã€ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã¯ãƒ¢ãƒ‡ãƒ«ã‚’GPU 0ã‹ã‚‰ä»–ã®GPUã«è¤‡è£½ã—ã¾ã™ã€‚\n",
      "- ãã‚Œã‹ã‚‰å„ãƒãƒƒãƒã”ã¨ã«:\n",
      "   1. å„GPUã¯å„è‡ªã®ãƒŸãƒ‹ãƒãƒƒãƒã®ãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥æ¶ˆè²»ã—ã¾ã™ã€‚\n",
      "   2. `backward`ä¸­ã€ãƒ­ãƒ¼ã‚«ãƒ«å‹¾é…ãŒæº–å‚™ã§ãã‚‹ã¨ã€ãã‚Œã‚‰ã¯ã™ã¹ã¦ã®ãƒ—ãƒ­ã‚»ã‚¹ã§å¹³å‡åŒ–ã•ã‚Œã¾ã™ã€‚\n",
      "\n",
      "[DP](https://pytorch.org/docs/master/generated/torch.nn.DataParallel.html):\n",
      "\n",
      "å„ãƒãƒƒãƒã”ã¨ã«:\n",
      "   1. GPU 0ã¯ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒã‚’èª­ã¿å–ã‚Šã€ãã‚Œã‹ã‚‰å„GPUã«ãƒŸãƒ‹ãƒãƒƒãƒã‚’é€ä¿¡ã—ã¾ã™ã€‚\n",
      "   2. GPU 0ã‹ã‚‰å„GPUã«æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¤‡è£½ã—ã¾ã™ã€‚\n",
      "   3. `forward`ã‚’å®Ÿè¡Œã—ã€å„GPUã‹ã‚‰GPU 0ã«å‡ºåŠ›ã‚’é€ä¿¡ã—ã€æå¤±ã‚’è¨ˆç®—ã—ã¾ã™ã€‚\n",
      "   4. GPU 0ã‹ã‚‰ã™ã¹ã¦ã®GPUã«æå¤±ã‚’åˆ†æ•£ã—ã€`backward`ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n",
      "   5. å„GPUã‹ã‚‰GPU 0ã«å‹¾é…ã‚’é€ä¿¡ã—ã€ãã‚Œã‚‰ã‚’å¹³å‡åŒ–ã—ã¾ã™ã€‚\n"
     ]
    }
   ],
   "source": [
    "print(elements[2000].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e79bc595-c7b4-4150-9d0e-a33d97afcd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def main():\n",
      "    # See all possible arguments in src/transformers/training_args.py\n",
      "    # or by passing the --help flag to this script.\n",
      "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
      "\n",
      "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
      "    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
      "        # If we pass only one argument to the script and it's the path to a json file,\n",
      "        # let's parse it to get our arguments.\n",
      "        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
      "    else:\n",
      "        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
      "\n",
      "    # Setup logging\n",
      "    logging.basicConfig(\n",
      "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
      "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
      "        handlers=[logging.StreamHandler(sys.stdout)],\n",
      "    )\n",
      "\n",
      "    if training_args.should_log:\n",
      "        # The default of training_args.log_level is passive, so we set log level at info here to have that default.\n",
      "        transformers.utils.logging.set_verbosity_info()\n",
      "\n",
      "    log_level = training_args.get_process_log_level()\n",
      "    logger.setLevel(log_level)\n",
      "    transformers.utils.logging.set_verbosity(log_level)\n",
      "    transformers.utils.logging.enable_default_handler()\n",
      "    transformers.utils.logging.enable_explicit_format()\n",
      "\n",
      "    # Log on each process the small summary:\n",
      "    logger.warning(\n",
      "        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n",
      "        + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n",
      "    )\n",
      "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
      "\n",
      "    # Set seed before initializing model.\n",
      "    set_seed(training_args.seed)\n",
      "\n",
      "    # Initialize our dataset and prepare it for the 'image-classification' task.\n",
      "    if data_args.dataset_name is not None:\n",
      "        dataset = load_dataset(\n",
      "            data_args.dataset_name,\n",
      "            data_args.dataset_config_name,\n",
      "            cache_dir=model_args.cache_dir,\n",
      "            token=model_args.token,\n",
      "            trust_remote_code=model_args.trust_remote_code,\n",
      "        )\n",
      "    else:\n",
      "        data_files = {}\n",
      "        if data_args.train_dir is not None:\n",
      "            data_files[\"train\"] = os.path.join(data_args.train_dir, \"**\")\n",
      "        if data_args.validation_dir is not None:\n",
      "            data_files[\"validation\"] = os.path.join(data_args.validation_dir, \"**\")\n",
      "        dataset = load_dataset(\n",
      "            \"imagefolder\",\n",
      "            data_files=data_files,\n",
      "            cache_dir=model_args.cache_dir,\n",
      "        )\n",
      "\n",
      "    dataset_column_names = dataset[\"train\"].column_names if \"train\" in dataset else dataset[\"validation\"].column_names\n",
      "    if data_args.image_column_name not in dataset_column_names:\n",
      "        raise ValueError(\n",
      "            f\"--image_column_name {data_args.image_column_name} not found in dataset '{data_args.dataset_name}'. \"\n",
      "            \"Make sure to set `--image_column_name` to the correct audio column - one of \"\n",
      "            f\"{', '.join(dataset_column_names)}.\"\n",
      "        )\n",
      "    if data_args.label_column_name not in dataset_column_names:\n",
      "        raise ValueError(\n",
      "            f\"--label_column_name {data_args.label_column_name} not found in dataset '{data_args.dataset_name}'. \"\n",
      "            \"Make sure to set `--label_column_name` to the correct text column - one of \"\n",
      "            f\"{', '.join(dataset_column_names)}.\"\n",
      "        )\n",
      "\n",
      "    def collate_fn(examples):\n",
      "        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
      "        labels = torch.tensor([example[data_args.label_column_name] for example in examples])\n",
      "        return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
      "\n",
      "    # If we don't have a validation split, split off a percentage of train as validation.\n",
      "    data_args.train_val_split = None if \"validation\" in dataset else data_args.train_val_split\n",
      "    if isinstance(data_args.train_val_split, float) and data_args.train_val_split > 0.0:\n",
      "        split = dataset[\"train\"].train_test_split(data_args.train_val_split)\n",
      "        dataset[\"train\"] = split[\"train\"]\n",
      "        dataset[\"validation\"] = split[\"test\"]\n",
      "\n",
      "    # Prepare label mappings.\n",
      "    # We'll include these in the model's config to get human readable labels in the Inference API.\n",
      "    labels = dataset[\"train\"].features[data_args.label_column_name].names\n",
      "    label2id, id2label = {}, {}\n",
      "    for i, label in enumerate(labels):\n",
      "        label2id[label] = str(i)\n",
      "        id2label[str(i)] = label\n",
      "\n",
      "    # Load the accuracy metric from the datasets package\n",
      "    metric = evaluate.load(\"accuracy\", cache_dir=model_args.cache_dir)\n",
      "\n",
      "    # Define our compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n",
      "    # predictions and label_ids field) and has to return a dictionary string to float.\n",
      "    def compute_metrics(p):\n",
      "        \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
      "        return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n",
      "\n",
      "    config = AutoConfig.from_pretrained(\n",
      "        model_args.config_name or model_args.model_name_or_path,\n",
      "        num_labels=len(labels),\n",
      "        label2id=label2id,\n",
      "        id2label=id2label,\n",
      "        finetuning_task=\"image-classification\",\n",
      "        cache_dir=model_args.cache_dir,\n",
      "        revision=model_args.model_revision,\n",
      "        token=model_args.token,\n",
      "        trust_remote_code=model_args.trust_remote_code,\n",
      "    )\n",
      "    model = AutoModelForImageClassification.from_pretrained(\n",
      "        model_args.model_name_or_path,\n",
      "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
      "        config=config,\n",
      "        cache_dir=model_args.cache_dir,\n",
      "        revision=model_args.model_revision,\n",
      "        token=model_args.token,\n",
      "        trust_remote_code=model_args.trust_remote_code,\n",
      "        ignore_mismatched_sizes=model_args.ignore_mismatched_sizes,\n",
      "    )\n",
      "    image_processor = AutoImageProcessor.from_pretrained(\n",
      "        model_args.image_processor_name or model_args.model_name_or_path,\n",
      "        cache_dir=model_args.cache_dir,\n",
      "        revision=model_args.model_revision,\n",
      "        token=model_args.token,\n",
      "        trust_remote_code=model_args.trust_remote_code,\n",
      "    )\n",
      "\n",
      "    # Define torchvision transforms to be applied to each image.\n",
      "    if isinstance(image_processor, TimmWrapperImageProcessor):\n",
      "        _train_transforms = image_processor.train_transforms\n",
      "        _val_transforms = image_processor.val_transforms\n",
      "    else:\n",
      "        if \"shortest_edge\" in image_processor.size:\n",
      "            size = image_processor.size[\"shortest_edge\"]\n",
      "        else:\n",
      "            size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
      "\n",
      "        # Create normalization transform\n",
      "        if hasattr(image_processor, \"image_mean\") and hasattr(image_processor, \"image_std\"):\n",
      "            normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
      "        else:\n",
      "            normalize = Lambda(lambda x: x)\n",
      "        _train_transforms = Compose(\n",
      "            [\n",
      "                RandomResizedCrop(size),\n",
      "                RandomHorizontalFlip(),\n",
      "                ToTensor(),\n",
      "                normalize,\n",
      "            ]\n",
      "        )\n",
      "        _val_transforms = Compose(\n",
      "            [\n",
      "                Resize(size),\n",
      "                CenterCrop(size),\n",
      "                ToTensor(),\n",
      "                normalize,\n",
      "            ]\n",
      "        )\n",
      "\n",
      "    def train_transforms(example_batch):\n",
      "        \"\"\"Apply _train_transforms across a batch.\"\"\"\n",
      "        example_batch[\"pixel_values\"] = [\n",
      "            _train_transforms(pil_img.convert(\"RGB\")) for pil_img in example_batch[data_args.image_column_name]\n",
      "        ]\n",
      "        return example_batch\n",
      "\n",
      "    def val_transforms(example_batch):\n",
      "        \"\"\"Apply _val_transforms across a batch.\"\"\"\n",
      "        example_batch[\"pixel_values\"] = [\n",
      "            _val_transforms(pil_img.convert(\"RGB\")) for pil_img in example_batch[data_args.image_column_name]\n",
      "        ]\n",
      "        return example_batch\n",
      "\n",
      "    if training_args.do_train:\n",
      "        if \"train\" not in dataset:\n",
      "            raise ValueError(\"--do_train requires a train dataset\")\n",
      "        if data_args.max_train_samples is not None:\n",
      "            dataset[\"train\"] = (\n",
      "                dataset[\"train\"].shuffle(seed=training_args.seed).select(range(data_args.max_train_samples))\n",
      "            )\n",
      "        # Set the training transforms\n",
      "        dataset[\"train\"].set_transform(train_transforms)\n",
      "\n",
      "    if training_args.do_eval:\n",
      "        if \"validation\" not in dataset:\n",
      "            raise ValueError(\"--do_eval requires a validation dataset\")\n",
      "        if data_args.max_eval_samples is not None:\n",
      "            dataset[\"validation\"] = (\n",
      "                dataset[\"validation\"].shuffle(seed=training_args.seed).select(range(data_args.max_eval_samples))\n",
      "            )\n",
      "        # Set the validation transforms\n",
      "        dataset[\"validation\"].set_transform(val_transforms)\n",
      "\n",
      "    # Initialize our trainer\n",
      "    trainer = Trainer(\n",
      "        model=model,\n",
      "        args=training_args,\n",
      "        train_dataset=dataset[\"train\"] if training_args.do_train else None,\n",
      "        eval_dataset=dataset[\"validation\"] if training_args.do_eval else None,\n",
      "        compute_metrics=compute_metrics,\n",
      "        processing_class=image_processor,\n",
      "        data_collator=collate_fn,\n",
      "    )\n",
      "\n",
      "    # Training\n",
      "    if training_args.do_train:\n",
      "        checkpoint = None\n",
      "        if training_args.resume_from_checkpoint is not None:\n",
      "            checkpoint = training_args.resume_from_checkpoint\n",
      "        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
      "        trainer.save_model()\n",
      "        trainer.log_metrics(\"train\", train_result.metrics)\n",
      "        trainer.save_metrics(\"train\", train_result.metrics)\n",
      "        trainer.save_state()\n",
      "\n",
      "    # Evaluation\n",
      "    if training_args.do_eval:\n",
      "        metrics = trainer.evaluate()\n",
      "        trainer.log_metrics(\"eval\", metrics)\n",
      "        trainer.save_metrics(\"eval\", metrics)\n",
      "\n",
      "    # Write model card and (optionally) push to hub\n",
      "    kwargs = {\n",
      "        \"finetuned_from\": model_args.model_name_or_path,\n",
      "        \"tasks\": \"image-classification\",\n",
      "        \"dataset\": data_args.dataset_name,\n",
      "        \"tags\": [\"image-classification\", \"vision\"],\n",
      "    }\n",
      "    if training_args.push_to_hub:\n",
      "        trainer.push_to_hub(**kwargs)\n",
      "    else:\n",
      "        trainer.create_model_card(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "print(elements[3000].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c93aed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"text\":\"def main():\\n    # See all possible arguments in src/transformers/training_args.py\\n    # or by passing the --help flag to this script.\\n    # We now keep distinct sets of args, for a cleaner separation of concerns.\\n\\n    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\\n    if len(sys.argv) == 2 and sys.argv[1].endswith(\\\".json\\\"):\\n        # If we pass only one argument to the script and it's the path to a json file,\\n        # let's parse it to get our arguments.\\n        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\\n    else:\\n        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\\n\\n    # Setup logging\\n    logging.basicConfig(\\n        format=\\\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\\\",\\n        datefmt=\\\"%m/%d/%Y %H:%M:%S\\\",\\n        handlers=[logging.StreamHandler(sys.stdout)],\\n    )\\n\\n    if training_args.should_log:\\n        # The default of training_args.log_level is passive, so we set log level at info here to have that default.\\n        transformers.utils.logging.set_verbosity_info()\\n\\n    log_level = training_args.get_process_log_level()\\n    logger.setLevel(log_level)\\n    transformers.utils.logging.set_verbosity(log_level)\\n    transformers.utils.logging.enable_default_handler()\\n    transformers.utils.logging.enable_explicit_format()\\n\\n    # Log on each process the small summary:\\n    logger.warning(\\n        f\\\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \\\"\\n        + f\\\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\\\"\\n    )\\n    logger.info(f\\\"Training/evaluation parameters {training_args}\\\")\\n\\n    # Set seed before initializing model.\\n    set_seed(training_args.seed)\\n\\n    # Initialize our dataset and prepare it for the 'image-classification' task.\\n    if data_args.dataset_name is not None:\\n        dataset = load_dataset(\\n            data_args.dataset_name,\\n            data_args.dataset_config_name,\\n            cache_dir=model_args.cache_dir,\\n            token=model_args.token,\\n            trust_remote_code=model_args.trust_remote_code,\\n        )\\n    else:\\n        data_files = {}\\n        if data_args.train_dir is not None:\\n            data_files[\\\"train\\\"] = os.path.join(data_args.train_dir, \\\"**\\\")\\n        if data_args.validation_dir is not None:\\n            data_files[\\\"validation\\\"] = os.path.join(data_args.validation_dir, \\\"**\\\")\\n        dataset = load_dataset(\\n            \\\"imagefolder\\\",\\n            data_files=data_files,\\n            cache_dir=model_args.cache_dir,\\n        )\\n\\n    dataset_column_names = dataset[\\\"train\\\"].column_names if \\\"train\\\" in dataset else dataset[\\\"validation\\\"].column_names\\n    if data_args.image_column_name not in dataset_column_names:\\n        raise ValueError(\\n            f\\\"--image_column_name {data_args.image_column_name} not found in dataset '{data_args.dataset_name}'. \\\"\\n            \\\"Make sure to set `--image_column_name` to the correct audio column - one of \\\"\\n            f\\\"{', '.join(dataset_column_names)}.\\\"\\n        )\\n    if data_args.label_column_name not in dataset_column_names:\\n        raise ValueError(\\n            f\\\"--label_column_name {data_args.label_column_name} not found in dataset '{data_args.dataset_name}'. \\\"\\n            \\\"Make sure to set `--label_column_name` to the correct text column - one of \\\"\\n            f\\\"{', '.join(dataset_column_names)}.\\\"\\n        )\\n\\n    def collate_fn(examples):\\n        pixel_values = torch.stack([example[\\\"pixel_values\\\"] for example in examples])\\n        labels = torch.tensor([example[data_args.label_column_name] for example in examples])\\n        return {\\\"pixel_values\\\": pixel_values, \\\"labels\\\": labels}\\n\\n    # If we don't have a validation split, split off a percentage of train as validation.\\n    data_args.train_val_split = None if \\\"validation\\\" in dataset else data_args.train_val_split\\n    if isinstance(data_args.train_val_split, float) and data_args.train_val_split > 0.0:\\n        split = dataset[\\\"train\\\"].train_test_split(data_args.train_val_split)\\n        dataset[\\\"train\\\"] = split[\\\"train\\\"]\\n        dataset[\\\"validation\\\"] = split[\\\"test\\\"]\\n\\n    # Prepare label mappings.\\n    # We'll include these in the model's config to get human readable labels in the Inference API.\\n    labels = dataset[\\\"train\\\"].features[data_args.label_column_name].names\\n    label2id, id2label = {}, {}\\n    for i, label in enumerate(labels):\\n        label2id[label] = str(i)\\n        id2label[str(i)] = label\\n\\n    # Load the accuracy metric from the datasets package\\n    metric = evaluate.load(\\\"accuracy\\\", cache_dir=model_args.cache_dir)\\n\\n    # Define our compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\\n    # predictions and label_ids field) and has to return a dictionary string to float.\\n    def compute_metrics(p):\\n        \\\"\\\"\\\"Computes accuracy on a batch of predictions\\\"\\\"\\\"\\n        return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\\n\\n    config = AutoConfig.from_pretrained(\\n        model_args.config_name or model_args.model_name_or_path,\\n        num_labels=len(labels),\\n        label2id=label2id,\\n        id2label=id2label,\\n        finetuning_task=\\\"image-classification\\\",\\n        cache_dir=model_args.cache_dir,\\n        revision=model_args.model_revision,\\n        token=model_args.token,\\n        trust_remote_code=model_args.trust_remote_code,\\n    )\\n    model = AutoModelForImageClassification.from_pretrained(\\n        model_args.model_name_or_path,\\n        from_tf=bool(\\\".ckpt\\\" in model_args.model_name_or_path),\\n        config=config,\\n        cache_dir=model_args.cache_dir,\\n        revision=model_args.model_revision,\\n        token=model_args.token,\\n        trust_remote_code=model_args.trust_remote_code,\\n        ignore_mismatched_sizes=model_args.ignore_mismatched_sizes,\\n    )\\n    image_processor = AutoImageProcessor.from_pretrained(\\n        model_args.image_processor_name or model_args.model_name_or_path,\\n        cache_dir=model_args.cache_dir,\\n        revision=model_args.model_revision,\\n        token=model_args.token,\\n        trust_remote_code=model_args.trust_remote_code,\\n    )\\n\\n    # Define torchvision transforms to be applied to each image.\\n    if isinstance(image_processor, TimmWrapperImageProcessor):\\n        _train_transforms = image_processor.train_transforms\\n        _val_transforms = image_processor.val_transforms\\n    else:\\n        if \\\"shortest_edge\\\" in image_processor.size:\\n            size = image_processor.size[\\\"shortest_edge\\\"]\\n        else:\\n            size = (image_processor.size[\\\"height\\\"], image_processor.size[\\\"width\\\"])\\n\\n        # Create normalization transform\\n        if hasattr(image_processor, \\\"image_mean\\\") and hasattr(image_processor, \\\"image_std\\\"):\\n            normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\\n        else:\\n            normalize = Lambda(lambda x: x)\\n        _train_transforms = Compose(\\n            [\\n                RandomResizedCrop(size),\\n                RandomHorizontalFlip(),\\n                ToTensor(),\\n                normalize,\\n            ]\\n        )\\n        _val_transforms = Compose(\\n            [\\n                Resize(size),\\n                CenterCrop(size),\\n                ToTensor(),\\n                normalize,\\n            ]\\n        )\\n\\n    def train_transforms(example_batch):\\n        \\\"\\\"\\\"Apply _train_transforms across a batch.\\\"\\\"\\\"\\n        example_batch[\\\"pixel_values\\\"] = [\\n            _train_transforms(pil_img.convert(\\\"RGB\\\")) for pil_img in example_batch[data_args.image_column_name]\\n        ]\\n        return example_batch\\n\\n    def val_transforms(example_batch):\\n        \\\"\\\"\\\"Apply _val_transforms across a batch.\\\"\\\"\\\"\\n        example_batch[\\\"pixel_values\\\"] = [\\n            _val_transforms(pil_img.convert(\\\"RGB\\\")) for pil_img in example_batch[data_args.image_column_name]\\n        ]\\n        return example_batch\\n\\n    if training_args.do_train:\\n        if \\\"train\\\" not in dataset:\\n            raise ValueError(\\\"--do_train requires a train dataset\\\")\\n        if data_args.max_train_samples is not None:\\n            dataset[\\\"train\\\"] = (\\n                dataset[\\\"train\\\"].shuffle(seed=training_args.seed).select(range(data_args.max_train_samples))\\n            )\\n        # Set the training transforms\\n        dataset[\\\"train\\\"].set_transform(train_transforms)\\n\\n    if training_args.do_eval:\\n        if \\\"validation\\\" not in dataset:\\n            raise ValueError(\\\"--do_eval requires a validation dataset\\\")\\n        if data_args.max_eval_samples is not None:\\n            dataset[\\\"validation\\\"] = (\\n                dataset[\\\"validation\\\"].shuffle(seed=training_args.seed).select(range(data_args.max_eval_samples))\\n            )\\n        # Set the validation transforms\\n        dataset[\\\"validation\\\"].set_transform(val_transforms)\\n\\n    # Initialize our trainer\\n    trainer = Trainer(\\n        model=model,\\n        args=training_args,\\n        train_dataset=dataset[\\\"train\\\"] if training_args.do_train else None,\\n        eval_dataset=dataset[\\\"validation\\\"] if training_args.do_eval else None,\\n        compute_metrics=compute_metrics,\\n        processing_class=image_processor,\\n        data_collator=collate_fn,\\n    )\\n\\n    # Training\\n    if training_args.do_train:\\n        checkpoint = None\\n        if training_args.resume_from_checkpoint is not None:\\n            checkpoint = training_args.resume_from_checkpoint\\n        train_result = trainer.train(resume_from_checkpoint=checkpoint)\\n        trainer.save_model()\\n        trainer.log_metrics(\\\"train\\\", train_result.metrics)\\n        trainer.save_metrics(\\\"train\\\", train_result.metrics)\\n        trainer.save_state()\\n\\n    # Evaluation\\n    if training_args.do_eval:\\n        metrics = trainer.evaluate()\\n        trainer.log_metrics(\\\"eval\\\", metrics)\\n        trainer.save_metrics(\\\"eval\\\", metrics)\\n\\n    # Write model card and (optionally) push to hub\\n    kwargs = {\\n        \\\"finetuned_from\\\": model_args.model_name_or_path,\\n        \\\"tasks\\\": \\\"image-classification\\\",\\n        \\\"dataset\\\": data_args.dataset_name,\\n        \\\"tags\\\": [\\\"image-classification\\\", \\\"vision\\\"],\\n    }\\n    if training_args.push_to_hub:\\n        trainer.push_to_hub(**kwargs)\\n    else:\\n        trainer.create_model_card(**kwargs)\",\"source\":\"examples/pytorch/image-classification/run_image_classification.py\",\"header\":\"import logging\\nimport os\\nimport sys\\nfrom dataclasses import dataclass, field\\nfrom typing import Optional\\nimport evaluate\\nimport numpy as np\\nimport torch\\nfrom datasets import load_dataset\\nfrom PIL import Image\\nfrom torchvision.transforms import (\\n    CenterCrop,\\n    Compose,\\n    Lambda,\\n    Normalize,\\n    RandomHorizontalFlip,\\n    RandomResizedCrop,\\n    Resize,\\n    ToTensor,\\n)\\nimport transformers\\nfrom transformers import (\\n    MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING,\\n    AutoConfig,\\n    AutoImageProcessor,\\n    AutoModelForImageClassification,\\n    HfArgumentParser,\\n    TimmWrapperImageProcessor,\\n    Trainer,\\n    TrainingArguments,\\n    set_seed,\\n)\\nfrom transformers.utils import check_min_version\\nfrom transformers.utils.versions import require_version\\n\\\"\\\"\\\" Fine-tuning a ğŸ¤— Transformers model for image classification\\\"\\\"\\\"\\nlogger = logging.getLogger(__name__)\\ncheck_min_version(\\\"4.57.0.dev0\\\")\\nrequire_version(\\\"datasets>=2.14.0\\\", \\\"To fix: pip install -r examples/pytorch/image-classification/requirements.txt\\\")\\nMODEL_CONFIG_CLASSES = list(MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING.keys())\\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\\nif __name__ == \\\"__main__\\\":\\n    main()\",\"extension\":\".py\",\"description\":\"\"}\n"
     ]
    }
   ],
   "source": [
    "print(elements[3000].model_dump_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b023db5f-d82f-4d1d-8ed1-519808faeab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.indexing.indexer import Indexer\n",
    "\n",
    "code_element = elements[3000]\n",
    "indexer = Indexer(None, None, None, None)\n",
    "summary = await indexer.summarize_element(code_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fff570fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This comprehensive Python script facilitates fine-tuning and evaluation of image classification models using the Hugging Face Transformers library, specifically targeting tasks within the vision domain. It provides a flexible command-line interface for configuring model parameters, dataset sources, training settings, and logging preferences, making it suitable for researchers and developers working on computer vision projects. The script employs the `HfArgumentParser` to parse arguments from either command-line inputs or JSON configuration files, enabling seamless customization of model, data, and training configurations through the `ModelArguments`, `DataTrainingArguments`, and `TrainingArguments` dataclasses. It sets up detailed logging and verbosity controls to monitor training progress and system information, including process rank, device type, GPU count, and precision mode, which are critical for distributed training scenarios. The dataset loading logic supports both remote datasets via the `datasets` library and local image folders, with validation for correct column names such as `image_column_name` and `label_column_name`. Data preprocessing involves applying torchvision transforms like resizing, cropping, normalization, and augmentation, tailored to the specific image processor used, whether standard or wrapped with Timm. The script dynamically splits datasets into training and validation subsets if necessary, and constructs label mappings for human-readable output. It integrates evaluation metrics like accuracy through the `evaluate` library, defining a `compute_metrics` function for model performance assessment. Model configuration and loading leverage `AutoConfig` and `AutoModelForImageClassification`, with support for remote or local models, while image preprocessing is handled via `AutoImageProcessor`. The training pipeline is managed by the `Trainer` class, which handles training, evaluation, checkpoint resumption, and model saving, with optional push to the Hugging Face Hub. Overall, this script encapsulates a complete workflow for training, evaluating, and deploying image classification models, making it a valuable resource for vision model development and experimentation within the Hugging Face ecosystem.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
