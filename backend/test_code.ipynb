{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2760a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.indexing.github_parsing import GitHubParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ee2d79d-772b-4b64-b4af-58d14d71ebe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/huggingface/transformers\"\n",
    "parser = GitHubParser(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aceb315a-d7eb-424e-8d1e-4e2f554322c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('huggingface', 'transformers', None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19476588-99e7-4f80-81f1-01dcd8428cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'huggingface'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(parser.owner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdf2cf24-3ceb-431f-ae2f-681940a3df86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'transformers'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(parser.repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9198fbf8-657f-42b6-93a3-f0d14a7e1002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(parser.ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7f3a4cd-41f9-4e44-b584-35111e5e40aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://codeload.github.com/huggingface/transformers/zip/master\n",
      "Response status code: 404\n",
      "URL: https://codeload.github.com/huggingface/transformers/zip/main\n",
      "Response status code: 200\n"
     ]
    }
   ],
   "source": [
    "zipb = parser.fetch_repo_zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8243fe4-6137-4b37-b638-37df5aa1eba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://codeload.github.com/huggingface/transformers/zip/master\n",
      "Response status code: 404\n",
      "URL: https://codeload.github.com/huggingface/transformers/zip/main\n",
      "Response status code: 200\n"
     ]
    }
   ],
   "source": [
    "from app.indexing.github_parsing import GitHubParser\n",
    "url = \"https://github.com/huggingface/transformers\"\n",
    "parser = GitHubParser(url)\n",
    "\n",
    "elements = parser.parse_repo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d597f2ae-c499-4a77-b8b1-0ea6f924d17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CodeElement(text='**Image Classification** \\n- [google/vit-base-patch16-224](https://huggingface.co/google/vit-base-patch16-224)\\n- [microsoft/beit-base-patch16-224-pt22k-ft22k](https://huggingface.co/microsoft/beit-base-patch16-224-pt22k-ft22k)\\n- [facebook/convnext-large-224](https://huggingface.co/facebook/convnext-large-224)\\n- [microsoft/resnet-50](https://huggingface.co/)\\n\\n**Image Segmentation** \\n- [nvidia/segformer-b0-finetuned-ade-512-512](https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512)\\n- [facebook/mask2former-swin-tiny-coco-panoptic](https://huggingface.co/facebook/mask2former-swin-tiny-coco-panoptic)\\n- [facebook/maskformer-swin-base-ade](https://huggingface.co/facebook/maskformer-swin-base-ade)\\n- [google/deeplabv3_mobilenet_v2_1.0_513](https://huggingface.co/google/deeplabv3_mobilenet_v2_1.0_513)\\n\\n**Object Detection** \\n- [google/owlvit-base-patch32](https://huggingface.co/google/owlvit-base-patch32)\\n- [facebook/detr-resnet-101](https://huggingface.co/facebook/detr-resnet-101)\\n- [microsoft/conditional-detr-resnet-50](https://huggingface.co/microsoft/conditional-detr-resnet-50)\\n\\n\\n以下は、`torch.compile()`を使用した場合と使用しない場合の推論時間の可視化と、異なるハードウェアとバッチサイズの各モデルに対するパフォーマンス向上の割合です。\\n\\n\\n<div class=\"flex\">\\n  <div>\\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/a100_batch_comp.png\" />\\n  </div>\\n  <div>\\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/v100_batch_comp.png\" />\\n  </div>\\n   <div>\\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/t4_batch_comp.png\" />\\n  </div>\\n</div>\\n\\n<div class=\"flex\">\\n  <div>\\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/A100_1_duration.png\" />\\n  </div>\\n  <div>\\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/A100_1_percentage.png\" />\\n  </div>\\n</div>\\n\\n![Duration Comparison on V100 with Batch Size of 1](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/v100_1_duration.png)\\n\\n![Percentage Improvement on T4 with Batch Size of 4](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/T4_4_percentage.png)\\n\\n下記は、各モデルについて`compile()`を使用した場合と使用しなかった場合の推論時間（ミリ秒単位）です。なお、OwlViTは大きなバッチサイズでの使用時にメモリ不足（OOM）が発生することに注意してください。\\n\\n### A100 (batch size: 1)\\n\\n| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |\\n|:---:|:---:|:---:|\\n| Image Classification/ViT | 9.325 | 7.584 | \\n| Image Segmentation/Segformer | 11.759 | 10.500 |\\n| Object Detection/OwlViT | 24.978 | 18.420 |\\n| Image Classification/BeiT | 11.282 | 8.448 | \\n| Object Detection/DETR | 34.619 | 19.040 |\\n| Image Classification/ConvNeXT | 10.410 | 10.208 | \\n| Image Classification/ResNet | 6.531 | 4.124 |\\n| Image Segmentation/Mask2former | 60.188 | 49.117 |\\n| Image Segmentation/Maskformer | 75.764 | 59.487 | \\n| Image Segmentation/MobileNet | 8.583 | 3.974 |\\n| Object Detection/Resnet-101 | 36.276 | 18.197 |\\n| Object Detection/Conditional-DETR | 31.219 | 17.993 |\\n\\n\\n### A100 (batch size: 4)\\n\\n| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |\\n|:---:|:---:|:---:|\\n| Image Classification/ViT | 14.832 | 14.499 | \\n| Image Segmentation/Segformer | 18.838 | 16.476 |\\n| Image Classification/BeiT | 13.205 | 13.048 | \\n| Object Detection/DETR | 48.657 | 32.418|\\n| Image Classification/ConvNeXT | 22.940 | 21.631 | \\n| Image Classification/ResNet | 6.657 | 4.268 |\\n| Image Segmentation/Mask2former | 74.277 | 61.781 |\\n| Image Segmentation/Maskformer | 180.700 | 159.116 | \\n| Image Segmentation/MobileNet | 14.174 | 8.515 |\\n| Object Detection/Resnet-101 | 68.101 | 44.998 |\\n| Object Detection/Conditional-DETR | 56.470 | 35.552 |\\n\\n### A100 (batch size: 16)\\n\\n| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |\\n|:---:|:---:|:---:|\\n| Image Classification/ViT | 40.944 | 40.010 | \\n| Image Segmentation/Segformer | 37.005 | 31.144 |\\n| Image Classification/BeiT | 41.854 | 41.048 | \\n| Object Detection/DETR | 164.382 | 161.902 |\\n| Image Classification/ConvNeXT | 82.258 | 75.561 | \\n| Image Classification/ResNet | 7.018 | 5.024 |\\n| Image Segmentation/Mask2former | 178.945 | 154.814 |\\n| Image Segmentation/Maskformer | 638.570 | 579.826 | \\n| Image Segmentation/MobileNet | 51.693 | 30.310 |\\n| Object Detection/Resnet-101 | 232.887 | 155.021 |\\n| Object Detection/Conditional-DETR | 180.491 | 124.032 |\\n\\n### V100 (batch size: 1)', source='docs/source/ja/perf_torch_compile.md', header='', extension='.md', description='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(elements[2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac1001da-5a7f-40d9-87c6-a13b96ab736a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Image Classification** \n",
      "- [google/vit-base-patch16-224](https://huggingface.co/google/vit-base-patch16-224)\n",
      "- [microsoft/beit-base-patch16-224-pt22k-ft22k](https://huggingface.co/microsoft/beit-base-patch16-224-pt22k-ft22k)\n",
      "- [facebook/convnext-large-224](https://huggingface.co/facebook/convnext-large-224)\n",
      "- [microsoft/resnet-50](https://huggingface.co/)\n",
      "\n",
      "**Image Segmentation** \n",
      "- [nvidia/segformer-b0-finetuned-ade-512-512](https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512)\n",
      "- [facebook/mask2former-swin-tiny-coco-panoptic](https://huggingface.co/facebook/mask2former-swin-tiny-coco-panoptic)\n",
      "- [facebook/maskformer-swin-base-ade](https://huggingface.co/facebook/maskformer-swin-base-ade)\n",
      "- [google/deeplabv3_mobilenet_v2_1.0_513](https://huggingface.co/google/deeplabv3_mobilenet_v2_1.0_513)\n",
      "\n",
      "**Object Detection** \n",
      "- [google/owlvit-base-patch32](https://huggingface.co/google/owlvit-base-patch32)\n",
      "- [facebook/detr-resnet-101](https://huggingface.co/facebook/detr-resnet-101)\n",
      "- [microsoft/conditional-detr-resnet-50](https://huggingface.co/microsoft/conditional-detr-resnet-50)\n",
      "\n",
      "\n",
      "以下は、`torch.compile()`を使用した場合と使用しない場合の推論時間の可視化と、異なるハードウェアとバッチサイズの各モデルに対するパフォーマンス向上の割合です。\n",
      "\n",
      "\n",
      "<div class=\"flex\">\n",
      "  <div>\n",
      "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/a100_batch_comp.png\" />\n",
      "  </div>\n",
      "  <div>\n",
      "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/v100_batch_comp.png\" />\n",
      "  </div>\n",
      "   <div>\n",
      "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/t4_batch_comp.png\" />\n",
      "  </div>\n",
      "</div>\n",
      "\n",
      "<div class=\"flex\">\n",
      "  <div>\n",
      "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/A100_1_duration.png\" />\n",
      "  </div>\n",
      "  <div>\n",
      "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/A100_1_percentage.png\" />\n",
      "  </div>\n",
      "</div>\n",
      "\n",
      "![Duration Comparison on V100 with Batch Size of 1](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/v100_1_duration.png)\n",
      "\n",
      "![Percentage Improvement on T4 with Batch Size of 4](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/T4_4_percentage.png)\n",
      "\n",
      "下記は、各モデルについて`compile()`を使用した場合と使用しなかった場合の推論時間（ミリ秒単位）です。なお、OwlViTは大きなバッチサイズでの使用時にメモリ不足（OOM）が発生することに注意してください。\n",
      "\n",
      "### A100 (batch size: 1)\n",
      "\n",
      "| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |\n",
      "|:---:|:---:|:---:|\n",
      "| Image Classification/ViT | 9.325 | 7.584 | \n",
      "| Image Segmentation/Segformer | 11.759 | 10.500 |\n",
      "| Object Detection/OwlViT | 24.978 | 18.420 |\n",
      "| Image Classification/BeiT | 11.282 | 8.448 | \n",
      "| Object Detection/DETR | 34.619 | 19.040 |\n",
      "| Image Classification/ConvNeXT | 10.410 | 10.208 | \n",
      "| Image Classification/ResNet | 6.531 | 4.124 |\n",
      "| Image Segmentation/Mask2former | 60.188 | 49.117 |\n",
      "| Image Segmentation/Maskformer | 75.764 | 59.487 | \n",
      "| Image Segmentation/MobileNet | 8.583 | 3.974 |\n",
      "| Object Detection/Resnet-101 | 36.276 | 18.197 |\n",
      "| Object Detection/Conditional-DETR | 31.219 | 17.993 |\n",
      "\n",
      "\n",
      "### A100 (batch size: 4)\n",
      "\n",
      "| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |\n",
      "|:---:|:---:|:---:|\n",
      "| Image Classification/ViT | 14.832 | 14.499 | \n",
      "| Image Segmentation/Segformer | 18.838 | 16.476 |\n",
      "| Image Classification/BeiT | 13.205 | 13.048 | \n",
      "| Object Detection/DETR | 48.657 | 32.418|\n",
      "| Image Classification/ConvNeXT | 22.940 | 21.631 | \n",
      "| Image Classification/ResNet | 6.657 | 4.268 |\n",
      "| Image Segmentation/Mask2former | 74.277 | 61.781 |\n",
      "| Image Segmentation/Maskformer | 180.700 | 159.116 | \n",
      "| Image Segmentation/MobileNet | 14.174 | 8.515 |\n",
      "| Object Detection/Resnet-101 | 68.101 | 44.998 |\n",
      "| Object Detection/Conditional-DETR | 56.470 | 35.552 |\n",
      "\n",
      "### A100 (batch size: 16)\n",
      "\n",
      "| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |\n",
      "|:---:|:---:|:---:|\n",
      "| Image Classification/ViT | 40.944 | 40.010 | \n",
      "| Image Segmentation/Segformer | 37.005 | 31.144 |\n",
      "| Image Classification/BeiT | 41.854 | 41.048 | \n",
      "| Object Detection/DETR | 164.382 | 161.902 |\n",
      "| Image Classification/ConvNeXT | 82.258 | 75.561 | \n",
      "| Image Classification/ResNet | 7.018 | 5.024 |\n",
      "| Image Segmentation/Mask2former | 178.945 | 154.814 |\n",
      "| Image Segmentation/Maskformer | 638.570 | 579.826 | \n",
      "| Image Segmentation/MobileNet | 51.693 | 30.310 |\n",
      "| Object Detection/Resnet-101 | 232.887 | 155.021 |\n",
      "| Object Detection/Conditional-DETR | 180.491 | 124.032 |\n",
      "\n",
      "### V100 (batch size: 1)\n"
     ]
    }
   ],
   "source": [
    "print(elements[2000].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e79bc595-c7b4-4150-9d0e-a33d97afcd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!---\n",
      "Copyright 2022 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "-->\n",
      "\n",
      "# VisionTextDualEncoder and CLIP model training examples\n",
      "\n",
      "The following example showcases how to train a CLIP-like vision-text dual encoder model\n",
      "using a pre-trained vision and text encoder.\n",
      "\n",
      "Such a model can be used for natural language image search and potentially zero-shot image classification.\n",
      "The model is inspired by [CLIP](https://openai.com/blog/clip/), introduced by Alec Radford et al.\n",
      "The idea is to train a vision encoder and a text encoder jointly to project the representation of images and their\n",
      "captions into the same embedding space, such that the caption embeddings are located near the embeddings\n",
      "of the images they describe.\n",
      "\n",
      "### Download COCO dataset (2017)\n",
      "This example uses COCO dataset (2017) through a custom dataset script, which requires users to manually download the\n",
      "COCO dataset before training.\n",
      "\n",
      "```bash\n",
      "mkdir data\n",
      "cd data\n",
      "wget http://images.cocodataset.org/zips/train2017.zip\n",
      "wget http://images.cocodataset.org/zips/val2017.zip\n",
      "wget http://images.cocodataset.org/zips/test2017.zip\n",
      "wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
      "wget http://images.cocodataset.org/annotations/image_info_test2017.zip\n",
      "cd ..\n",
      "```\n",
      "\n",
      "Having downloaded COCO dataset manually you should be able to load with the `ydshieh/coc_dataset_script` dataset loading script:\n",
      "\n",
      "```py\n",
      "import os\n",
      "import datasets\n",
      "\n",
      "COCO_DIR = os.path.join(os.getcwd(), \"data\")\n",
      "ds = datasets.load_dataset(\"ydshieh/coco_dataset_script\", \"2017\", data_dir=COCO_DIR)\n",
      "```\n",
      "\n",
      "### Create a model from a vision encoder model and a text encoder model\n",
      "Next, we create a [VisionTextDualEncoderModel](https://huggingface.co/docs/transformers/model_doc/vision-text-dual-encoder#visiontextdualencoder).\n",
      "The `VisionTextDualEncoderModel` class lets you load any vision and text encoder model to create a dual encoder.\n",
      "Here is an example of how to load the model using pre-trained vision and text models.\n",
      "\n",
      "```python3\n",
      "from transformers import (\n",
      "    VisionTextDualEncoderModel,\n",
      "    VisionTextDualEncoderProcessor,\n",
      "    AutoTokenizer,\n",
      "    AutoImageProcessor\n",
      ")\n",
      "\n",
      "model = VisionTextDualEncoderModel.from_vision_text_pretrained(\n",
      "    \"openai/clip-vit-base-patch32\", \"FacebookAI/roberta-base\"\n",
      ")\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
      "image_processor = AutoImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
      "processor = VisionTextDualEncoderProcessor(image_processor, tokenizer)\n",
      "\n",
      "# save the model and processor\n",
      "model.save_pretrained(\"clip-roberta\")\n",
      "processor.save_pretrained(\"clip-roberta\")\n",
      "```\n",
      "\n",
      "This loads both the text and vision encoders using pre-trained weights, the projection layers are randomly\n",
      "initialized except for CLIP's vision model. If you use CLIP to initialize the vision model then the vision projection weights are also\n",
      "loaded using the pre-trained weights.\n",
      "\n",
      "### Train the model\n",
      "Finally, we can run the example script to train the model:\n",
      "\n",
      "```bash\n",
      "python run_clip.py \\\n",
      "    --output_dir ./clip-roberta-finetuned \\\n",
      "    --model_name_or_path ./clip-roberta \\\n",
      "    --data_dir $PWD/data \\\n",
      "    --dataset_name ydshieh/coco_dataset_script \\\n",
      "    --dataset_config_name=2017 \\\n",
      "    --image_column image_path \\\n",
      "    --caption_column caption \\\n",
      "    --remove_unused_columns=False \\\n",
      "    --do_train  --do_eval \\\n",
      "    --per_device_train_batch_size=\"64\" \\\n",
      "    --per_device_eval_batch_size=\"64\" \\\n",
      "    --learning_rate=\"5e-5\" --warmup_steps=\"0\" --weight_decay 0.1 \\\n",
      "    --overwrite_output_dir \\\n"
     ]
    }
   ],
   "source": [
    "print(elements[3000].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b023db5f-d82f-4d1d-8ed1-519808faeab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
