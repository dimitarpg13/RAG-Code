{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2760a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.indexing.github_parsing import GitHubParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ee2d79d-772b-4b64-b4af-58d14d71ebe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/huggingface/transformers\"\n",
    "parser = GitHubParser(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aceb315a-d7eb-424e-8d1e-4e2f554322c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('huggingface', 'transformers', None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19476588-99e7-4f80-81f1-01dcd8428cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'huggingface'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(parser.owner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdf2cf24-3ceb-431f-ae2f-681940a3df86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'transformers'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(parser.repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9198fbf8-657f-42b6-93a3-f0d14a7e1002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(parser.ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7f3a4cd-41f9-4e44-b584-35111e5e40aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://codeload.github.com/huggingface/transformers/zip/master\n",
      "Response status code: 404\n",
      "URL: https://codeload.github.com/huggingface/transformers/zip/main\n",
      "Response status code: 200\n"
     ]
    }
   ],
   "source": [
    "zipb = parser.fetch_repo_zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8243fe4-6137-4b37-b638-37df5aa1eba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://codeload.github.com/huggingface/transformers/zip/master\n",
      "Response status code: 404\n",
      "URL: https://codeload.github.com/huggingface/transformers/zip/main\n",
      "Response status code: 200\n"
     ]
    }
   ],
   "source": [
    "from app.indexing.github_parsing import GitHubParser\n",
    "url = \"https://github.com/huggingface/transformers\"\n",
    "parser = GitHubParser(url)\n",
    "\n",
    "elements = parser.parse_repo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d597f2ae-c499-4a77-b8b1-0ea6f924d17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CodeElement(text='<!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Efficient Training on Multiple GPUs\\n\\n単一のGPUでのトレーニングが遅すぎる場合や、モデルの重みが単一のGPUのメモリに収まらない場合、複数のGPUを使用したセットアップが必要となります。単一のGPUから複数のGPUへの切り替えには、ワークロードを分散するためのある種の並列処理が必要です。データ、テンソル、またはパイプラインの並列処理など、さまざまな並列処理技術があります。ただし、すべてに適した一つの解決策は存在せず、最適な設定は使用するハードウェアに依存します。この記事は、おそらく他のフレームワークにも適用される主要な概念に焦点を当てつつ、PyTorchベースの実装に焦点を当てています。\\n\\n<Tip>\\n\\n**注意**: [単一GPUセクション](perf_train_gpu_one) で紹介された多くの戦略（混合精度トレーニングや勾配蓄積など）は一般的であり、モデルのトレーニングに一般的に適用されます。したがって、マルチGPUやCPUトレーニングなどの次のセクションに入る前に、それを確認してください。\\n\\n</Tip>\\n\\nまず、さまざまな1D並列処理技術とその利点および欠点について詳しく説明し、それらを2Dおよび3D並列処理に組み合わせてさらに高速なトレーニングを実現し、より大きなモデルをサポートする方法を検討します。さまざまな他の強力な代替手法も紹介されます。\\n\\n## Concepts\\n\\n以下は、この文書で後で詳しく説明される主要な概念の簡単な説明です。\\n\\n1. **DataParallel (DP)** - 同じセットアップが複数回複製され、各セットアップにデータのスライスが供給されます。処理は並行して行われ、各セットアップはトレーニングステップの最後に同期されます。\\n2. **TensorParallel (TP)** - 各テンソルは複数のチャンクに分割され、単一のGPUにテンソル全体が存在するのではなく、テンソルの各シャードが指定されたGPUに存在します。処理中に、各シャードは別々に並行して処理され、異なるGPUで同期され、ステップの最後に結果が同期されます。これは水平並列処理と呼ばれるもので、分割は水平レベルで行われます。\\n3. **PipelineParallel (PP)** - モデルは垂直（レイヤーレベル）に複数のGPUに分割され、モデルの単一または複数のレイヤーが単一のGPUに配置されます。各GPUはパイプラインの異なるステージを並行して処理し、バッチの小さなチャンクで作業します。\\n4. **Zero Redundancy Optimizer (ZeRO)** - TPといくらか似たようなテンソルのシャーディングを実行しますが、前向きまたは後向きの計算のためにテンソル全体が再構築されるため、モデルを変更する必要はありません。また、GPUメモリが制限されている場合に補償するためのさまざまなオフロード技術をサポートします。\\n5. **Sharded DDP** - Sharded DDPは、さまざまなZeRO実装で使用される基本的なZeROコンセプトの別名です。\\n\\n各コンセプトの詳細に深入りする前に、大規模なインフラストラクチャで大規模なモデルをトレーニングする際の大まかな決定プロセスを見てみましょう。\\n\\n## Scalability Strategy\\n\\n**⇨ シングルノード / マルチGPU**\\n* モデルが単一のGPUに収まる場合：\\n\\n    1. DDP - 分散データ並列\\n    2. ZeRO - 状況と使用される構成に応じて速いかどうかが異なります\\n\\n* モデルが単一のGPUに収まらない場合：\\n\\n    1. PP\\n    2. ZeRO\\n    3. TP\\n\\n    非常に高速なノード内接続（NVLINKまたはNVSwitchなど）があれば、これらの3つはほぼ同じ速度になるはずで、これらがない場合、PPはTPまたはZeROよりも速くなります。TPの程度も差を生じるかもしれません。特定のセットアップでの勝者を見つけるために実験することが最善です。\\n\\n    TPはほとんどの場合、単一ノード内で使用されます。つまり、TPサイズ <= ノードごとのGPU数です。\\n\\n* 最大のレイヤーが単一のGPUに収まらない場合：\\n\\n    1. ZeROを使用しない場合 - TPを使用する必要があります。PP単独では収まらないでしょう。\\n    2. ZeROを使用する場合 - \"シングルGPU\"のエントリと同じものを参照してください\\n\\n**⇨ マルチノード / マルチGPU**\\n\\n* ノード間の高速接続がある場合：\\n\\n    1. ZeRO - モデルへのほとんどの変更が不要です\\n    2. PP+TP+DP - 通信が少なく、モデルへの大規模な変更が必要です\\n\\n* ノード間の接続が遅く、GPUメモリがまだ不足している場合：\\n\\n    1. DP+PP+TP+ZeRO-1\\n\\n## Data Parallelism\\n\\n2つのGPUを持つほとんどのユーザーは、`DataParallel`（DP）と`DistributedDataParallel`（DDP）によって提供されるトレーニング速度の向上をすでに享受しています。これらはほぼ自明に使用できるPyTorchの組み込み機能です。一般的に、すべてのモデルで動作するDDPを使用することをお勧めします。DPは一部のモデルで失敗する可能性があるためです。[PyTorchのドキュメンテーション](https://pytorch.org/docs/master/generated/torch.nn.DataParallel.html)自体もDDPの使用を推奨しています。\\n\\n### DP vs DDP\\n\\n`DistributedDataParallel`（DDP）は通常、`DataParallel`（DP）よりも高速ですが、常にそうとは限りません：\\n* DPはPythonスレッドベースですが、DDPはマルチプロセスベースです。そのため、GIL（Global Interpreter Lock）などのPythonスレッドの制約がないためです。\\n* 一方、GPUカード間の遅い相互接続性は、DDPの場合に実際には遅い結果をもたらす可能性があります。\\n\\n以下は、2つのモード間のGPU間通信の主な違いです：\\n\\n[DDP](https://pytorch.org/docs/master/notes/ddp.html):\\n\\n- 開始時、メインプロセスはモデルをGPU 0から他のGPUに複製します。\\n- それから各バッチごとに:\\n   1. 各GPUは各自のミニバッチのデータを直接消費します。\\n   2. `backward`中、ローカル勾配が準備できると、それらはすべてのプロセスで平均化されます。\\n\\n[DP](https://pytorch.org/docs/master/generated/torch.nn.DataParallel.html):\\n\\n各バッチごとに:\\n   1. GPU 0はデータバッチを読み取り、それから各GPUにミニバッチを送信します。\\n   2. GPU 0から各GPUに最新のモデルを複製します。\\n   3. `forward`を実行し、各GPUからGPU 0に出力を送信し、損失を計算します。\\n   4. GPU 0からすべてのGPUに損失を分散し、`backward`を実行します。\\n   5. 各GPUからGPU 0に勾配を送信し、それらを平均化します。', source='docs/source/ja/perf_train_gpu_many.md', header='', extension='.md', description='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(elements[2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac1001da-5a7f-40d9-87c6-a13b96ab736a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "\n",
      "⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n",
      "rendered properly in your Markdown viewer.\n",
      "\n",
      "-->\n",
      "\n",
      "# Efficient Training on Multiple GPUs\n",
      "\n",
      "単一のGPUでのトレーニングが遅すぎる場合や、モデルの重みが単一のGPUのメモリに収まらない場合、複数のGPUを使用したセットアップが必要となります。単一のGPUから複数のGPUへの切り替えには、ワークロードを分散するためのある種の並列処理が必要です。データ、テンソル、またはパイプラインの並列処理など、さまざまな並列処理技術があります。ただし、すべてに適した一つの解決策は存在せず、最適な設定は使用するハードウェアに依存します。この記事は、おそらく他のフレームワークにも適用される主要な概念に焦点を当てつつ、PyTorchベースの実装に焦点を当てています。\n",
      "\n",
      "<Tip>\n",
      "\n",
      "**注意**: [単一GPUセクション](perf_train_gpu_one) で紹介された多くの戦略（混合精度トレーニングや勾配蓄積など）は一般的であり、モデルのトレーニングに一般的に適用されます。したがって、マルチGPUやCPUトレーニングなどの次のセクションに入る前に、それを確認してください。\n",
      "\n",
      "</Tip>\n",
      "\n",
      "まず、さまざまな1D並列処理技術とその利点および欠点について詳しく説明し、それらを2Dおよび3D並列処理に組み合わせてさらに高速なトレーニングを実現し、より大きなモデルをサポートする方法を検討します。さまざまな他の強力な代替手法も紹介されます。\n",
      "\n",
      "## Concepts\n",
      "\n",
      "以下は、この文書で後で詳しく説明される主要な概念の簡単な説明です。\n",
      "\n",
      "1. **DataParallel (DP)** - 同じセットアップが複数回複製され、各セットアップにデータのスライスが供給されます。処理は並行して行われ、各セットアップはトレーニングステップの最後に同期されます。\n",
      "2. **TensorParallel (TP)** - 各テンソルは複数のチャンクに分割され、単一のGPUにテンソル全体が存在するのではなく、テンソルの各シャードが指定されたGPUに存在します。処理中に、各シャードは別々に並行して処理され、異なるGPUで同期され、ステップの最後に結果が同期されます。これは水平並列処理と呼ばれるもので、分割は水平レベルで行われます。\n",
      "3. **PipelineParallel (PP)** - モデルは垂直（レイヤーレベル）に複数のGPUに分割され、モデルの単一または複数のレイヤーが単一のGPUに配置されます。各GPUはパイプラインの異なるステージを並行して処理し、バッチの小さなチャンクで作業します。\n",
      "4. **Zero Redundancy Optimizer (ZeRO)** - TPといくらか似たようなテンソルのシャーディングを実行しますが、前向きまたは後向きの計算のためにテンソル全体が再構築されるため、モデルを変更する必要はありません。また、GPUメモリが制限されている場合に補償するためのさまざまなオフロード技術をサポートします。\n",
      "5. **Sharded DDP** - Sharded DDPは、さまざまなZeRO実装で使用される基本的なZeROコンセプトの別名です。\n",
      "\n",
      "各コンセプトの詳細に深入りする前に、大規模なインフラストラクチャで大規模なモデルをトレーニングする際の大まかな決定プロセスを見てみましょう。\n",
      "\n",
      "## Scalability Strategy\n",
      "\n",
      "**⇨ シングルノード / マルチGPU**\n",
      "* モデルが単一のGPUに収まる場合：\n",
      "\n",
      "    1. DDP - 分散データ並列\n",
      "    2. ZeRO - 状況と使用される構成に応じて速いかどうかが異なります\n",
      "\n",
      "* モデルが単一のGPUに収まらない場合：\n",
      "\n",
      "    1. PP\n",
      "    2. ZeRO\n",
      "    3. TP\n",
      "\n",
      "    非常に高速なノード内接続（NVLINKまたはNVSwitchなど）があれば、これらの3つはほぼ同じ速度になるはずで、これらがない場合、PPはTPまたはZeROよりも速くなります。TPの程度も差を生じるかもしれません。特定のセットアップでの勝者を見つけるために実験することが最善です。\n",
      "\n",
      "    TPはほとんどの場合、単一ノード内で使用されます。つまり、TPサイズ <= ノードごとのGPU数です。\n",
      "\n",
      "* 最大のレイヤーが単一のGPUに収まらない場合：\n",
      "\n",
      "    1. ZeROを使用しない場合 - TPを使用する必要があります。PP単独では収まらないでしょう。\n",
      "    2. ZeROを使用する場合 - \"シングルGPU\"のエントリと同じものを参照してください\n",
      "\n",
      "**⇨ マルチノード / マルチGPU**\n",
      "\n",
      "* ノード間の高速接続がある場合：\n",
      "\n",
      "    1. ZeRO - モデルへのほとんどの変更が不要です\n",
      "    2. PP+TP+DP - 通信が少なく、モデルへの大規模な変更が必要です\n",
      "\n",
      "* ノード間の接続が遅く、GPUメモリがまだ不足している場合：\n",
      "\n",
      "    1. DP+PP+TP+ZeRO-1\n",
      "\n",
      "## Data Parallelism\n",
      "\n",
      "2つのGPUを持つほとんどのユーザーは、`DataParallel`（DP）と`DistributedDataParallel`（DDP）によって提供されるトレーニング速度の向上をすでに享受しています。これらはほぼ自明に使用できるPyTorchの組み込み機能です。一般的に、すべてのモデルで動作するDDPを使用することをお勧めします。DPは一部のモデルで失敗する可能性があるためです。[PyTorchのドキュメンテーション](https://pytorch.org/docs/master/generated/torch.nn.DataParallel.html)自体もDDPの使用を推奨しています。\n",
      "\n",
      "### DP vs DDP\n",
      "\n",
      "`DistributedDataParallel`（DDP）は通常、`DataParallel`（DP）よりも高速ですが、常にそうとは限りません：\n",
      "* DPはPythonスレッドベースですが、DDPはマルチプロセスベースです。そのため、GIL（Global Interpreter Lock）などのPythonスレッドの制約がないためです。\n",
      "* 一方、GPUカード間の遅い相互接続性は、DDPの場合に実際には遅い結果をもたらす可能性があります。\n",
      "\n",
      "以下は、2つのモード間のGPU間通信の主な違いです：\n",
      "\n",
      "[DDP](https://pytorch.org/docs/master/notes/ddp.html):\n",
      "\n",
      "- 開始時、メインプロセスはモデルをGPU 0から他のGPUに複製します。\n",
      "- それから各バッチごとに:\n",
      "   1. 各GPUは各自のミニバッチのデータを直接消費します。\n",
      "   2. `backward`中、ローカル勾配が準備できると、それらはすべてのプロセスで平均化されます。\n",
      "\n",
      "[DP](https://pytorch.org/docs/master/generated/torch.nn.DataParallel.html):\n",
      "\n",
      "各バッチごとに:\n",
      "   1. GPU 0はデータバッチを読み取り、それから各GPUにミニバッチを送信します。\n",
      "   2. GPU 0から各GPUに最新のモデルを複製します。\n",
      "   3. `forward`を実行し、各GPUからGPU 0に出力を送信し、損失を計算します。\n",
      "   4. GPU 0からすべてのGPUに損失を分散し、`backward`を実行します。\n",
      "   5. 各GPUからGPU 0に勾配を送信し、それらを平均化します。\n"
     ]
    }
   ],
   "source": [
    "print(elements[2000].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e79bc595-c7b4-4150-9d0e-a33d97afcd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def main():\n",
      "    # See all possible arguments in src/transformers/training_args.py\n",
      "    # or by passing the --help flag to this script.\n",
      "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
      "\n",
      "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
      "    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
      "        # If we pass only one argument to the script and it's the path to a json file,\n",
      "        # let's parse it to get our arguments.\n",
      "        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
      "    else:\n",
      "        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
      "\n",
      "    # Setup logging\n",
      "    logging.basicConfig(\n",
      "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
      "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
      "        handlers=[logging.StreamHandler(sys.stdout)],\n",
      "    )\n",
      "\n",
      "    if training_args.should_log:\n",
      "        # The default of training_args.log_level is passive, so we set log level at info here to have that default.\n",
      "        transformers.utils.logging.set_verbosity_info()\n",
      "\n",
      "    log_level = training_args.get_process_log_level()\n",
      "    logger.setLevel(log_level)\n",
      "    transformers.utils.logging.set_verbosity(log_level)\n",
      "    transformers.utils.logging.enable_default_handler()\n",
      "    transformers.utils.logging.enable_explicit_format()\n",
      "\n",
      "    # Log on each process the small summary:\n",
      "    logger.warning(\n",
      "        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n",
      "        + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n",
      "    )\n",
      "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
      "\n",
      "    # Set seed before initializing model.\n",
      "    set_seed(training_args.seed)\n",
      "\n",
      "    # Initialize our dataset and prepare it for the 'image-classification' task.\n",
      "    if data_args.dataset_name is not None:\n",
      "        dataset = load_dataset(\n",
      "            data_args.dataset_name,\n",
      "            data_args.dataset_config_name,\n",
      "            cache_dir=model_args.cache_dir,\n",
      "            token=model_args.token,\n",
      "            trust_remote_code=model_args.trust_remote_code,\n",
      "        )\n",
      "    else:\n",
      "        data_files = {}\n",
      "        if data_args.train_dir is not None:\n",
      "            data_files[\"train\"] = os.path.join(data_args.train_dir, \"**\")\n",
      "        if data_args.validation_dir is not None:\n",
      "            data_files[\"validation\"] = os.path.join(data_args.validation_dir, \"**\")\n",
      "        dataset = load_dataset(\n",
      "            \"imagefolder\",\n",
      "            data_files=data_files,\n",
      "            cache_dir=model_args.cache_dir,\n",
      "        )\n",
      "\n",
      "    dataset_column_names = dataset[\"train\"].column_names if \"train\" in dataset else dataset[\"validation\"].column_names\n",
      "    if data_args.image_column_name not in dataset_column_names:\n",
      "        raise ValueError(\n",
      "            f\"--image_column_name {data_args.image_column_name} not found in dataset '{data_args.dataset_name}'. \"\n",
      "            \"Make sure to set `--image_column_name` to the correct audio column - one of \"\n",
      "            f\"{', '.join(dataset_column_names)}.\"\n",
      "        )\n",
      "    if data_args.label_column_name not in dataset_column_names:\n",
      "        raise ValueError(\n",
      "            f\"--label_column_name {data_args.label_column_name} not found in dataset '{data_args.dataset_name}'. \"\n",
      "            \"Make sure to set `--label_column_name` to the correct text column - one of \"\n",
      "            f\"{', '.join(dataset_column_names)}.\"\n",
      "        )\n",
      "\n",
      "    def collate_fn(examples):\n",
      "        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
      "        labels = torch.tensor([example[data_args.label_column_name] for example in examples])\n",
      "        return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
      "\n",
      "    # If we don't have a validation split, split off a percentage of train as validation.\n",
      "    data_args.train_val_split = None if \"validation\" in dataset else data_args.train_val_split\n",
      "    if isinstance(data_args.train_val_split, float) and data_args.train_val_split > 0.0:\n",
      "        split = dataset[\"train\"].train_test_split(data_args.train_val_split)\n",
      "        dataset[\"train\"] = split[\"train\"]\n",
      "        dataset[\"validation\"] = split[\"test\"]\n",
      "\n",
      "    # Prepare label mappings.\n",
      "    # We'll include these in the model's config to get human readable labels in the Inference API.\n",
      "    labels = dataset[\"train\"].features[data_args.label_column_name].names\n",
      "    label2id, id2label = {}, {}\n",
      "    for i, label in enumerate(labels):\n",
      "        label2id[label] = str(i)\n",
      "        id2label[str(i)] = label\n",
      "\n",
      "    # Load the accuracy metric from the datasets package\n",
      "    metric = evaluate.load(\"accuracy\", cache_dir=model_args.cache_dir)\n",
      "\n",
      "    # Define our compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n",
      "    # predictions and label_ids field) and has to return a dictionary string to float.\n",
      "    def compute_metrics(p):\n",
      "        \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
      "        return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n",
      "\n",
      "    config = AutoConfig.from_pretrained(\n",
      "        model_args.config_name or model_args.model_name_or_path,\n",
      "        num_labels=len(labels),\n",
      "        label2id=label2id,\n",
      "        id2label=id2label,\n",
      "        finetuning_task=\"image-classification\",\n",
      "        cache_dir=model_args.cache_dir,\n",
      "        revision=model_args.model_revision,\n",
      "        token=model_args.token,\n",
      "        trust_remote_code=model_args.trust_remote_code,\n",
      "    )\n",
      "    model = AutoModelForImageClassification.from_pretrained(\n",
      "        model_args.model_name_or_path,\n",
      "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
      "        config=config,\n",
      "        cache_dir=model_args.cache_dir,\n",
      "        revision=model_args.model_revision,\n",
      "        token=model_args.token,\n",
      "        trust_remote_code=model_args.trust_remote_code,\n",
      "        ignore_mismatched_sizes=model_args.ignore_mismatched_sizes,\n",
      "    )\n",
      "    image_processor = AutoImageProcessor.from_pretrained(\n",
      "        model_args.image_processor_name or model_args.model_name_or_path,\n",
      "        cache_dir=model_args.cache_dir,\n",
      "        revision=model_args.model_revision,\n",
      "        token=model_args.token,\n",
      "        trust_remote_code=model_args.trust_remote_code,\n",
      "    )\n",
      "\n",
      "    # Define torchvision transforms to be applied to each image.\n",
      "    if isinstance(image_processor, TimmWrapperImageProcessor):\n",
      "        _train_transforms = image_processor.train_transforms\n",
      "        _val_transforms = image_processor.val_transforms\n",
      "    else:\n",
      "        if \"shortest_edge\" in image_processor.size:\n",
      "            size = image_processor.size[\"shortest_edge\"]\n",
      "        else:\n",
      "            size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
      "\n",
      "        # Create normalization transform\n",
      "        if hasattr(image_processor, \"image_mean\") and hasattr(image_processor, \"image_std\"):\n",
      "            normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
      "        else:\n",
      "            normalize = Lambda(lambda x: x)\n",
      "        _train_transforms = Compose(\n",
      "            [\n",
      "                RandomResizedCrop(size),\n",
      "                RandomHorizontalFlip(),\n",
      "                ToTensor(),\n",
      "                normalize,\n",
      "            ]\n",
      "        )\n",
      "        _val_transforms = Compose(\n",
      "            [\n",
      "                Resize(size),\n",
      "                CenterCrop(size),\n",
      "                ToTensor(),\n",
      "                normalize,\n",
      "            ]\n",
      "        )\n",
      "\n",
      "    def train_transforms(example_batch):\n",
      "        \"\"\"Apply _train_transforms across a batch.\"\"\"\n",
      "        example_batch[\"pixel_values\"] = [\n",
      "            _train_transforms(pil_img.convert(\"RGB\")) for pil_img in example_batch[data_args.image_column_name]\n",
      "        ]\n",
      "        return example_batch\n",
      "\n",
      "    def val_transforms(example_batch):\n",
      "        \"\"\"Apply _val_transforms across a batch.\"\"\"\n",
      "        example_batch[\"pixel_values\"] = [\n",
      "            _val_transforms(pil_img.convert(\"RGB\")) for pil_img in example_batch[data_args.image_column_name]\n",
      "        ]\n",
      "        return example_batch\n",
      "\n",
      "    if training_args.do_train:\n",
      "        if \"train\" not in dataset:\n",
      "            raise ValueError(\"--do_train requires a train dataset\")\n",
      "        if data_args.max_train_samples is not None:\n",
      "            dataset[\"train\"] = (\n",
      "                dataset[\"train\"].shuffle(seed=training_args.seed).select(range(data_args.max_train_samples))\n",
      "            )\n",
      "        # Set the training transforms\n",
      "        dataset[\"train\"].set_transform(train_transforms)\n",
      "\n",
      "    if training_args.do_eval:\n",
      "        if \"validation\" not in dataset:\n",
      "            raise ValueError(\"--do_eval requires a validation dataset\")\n",
      "        if data_args.max_eval_samples is not None:\n",
      "            dataset[\"validation\"] = (\n",
      "                dataset[\"validation\"].shuffle(seed=training_args.seed).select(range(data_args.max_eval_samples))\n",
      "            )\n",
      "        # Set the validation transforms\n",
      "        dataset[\"validation\"].set_transform(val_transforms)\n",
      "\n",
      "    # Initialize our trainer\n",
      "    trainer = Trainer(\n",
      "        model=model,\n",
      "        args=training_args,\n",
      "        train_dataset=dataset[\"train\"] if training_args.do_train else None,\n",
      "        eval_dataset=dataset[\"validation\"] if training_args.do_eval else None,\n",
      "        compute_metrics=compute_metrics,\n",
      "        processing_class=image_processor,\n",
      "        data_collator=collate_fn,\n",
      "    )\n",
      "\n",
      "    # Training\n",
      "    if training_args.do_train:\n",
      "        checkpoint = None\n",
      "        if training_args.resume_from_checkpoint is not None:\n",
      "            checkpoint = training_args.resume_from_checkpoint\n",
      "        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
      "        trainer.save_model()\n",
      "        trainer.log_metrics(\"train\", train_result.metrics)\n",
      "        trainer.save_metrics(\"train\", train_result.metrics)\n",
      "        trainer.save_state()\n",
      "\n",
      "    # Evaluation\n",
      "    if training_args.do_eval:\n",
      "        metrics = trainer.evaluate()\n",
      "        trainer.log_metrics(\"eval\", metrics)\n",
      "        trainer.save_metrics(\"eval\", metrics)\n",
      "\n",
      "    # Write model card and (optionally) push to hub\n",
      "    kwargs = {\n",
      "        \"finetuned_from\": model_args.model_name_or_path,\n",
      "        \"tasks\": \"image-classification\",\n",
      "        \"dataset\": data_args.dataset_name,\n",
      "        \"tags\": [\"image-classification\", \"vision\"],\n",
      "    }\n",
      "    if training_args.push_to_hub:\n",
      "        trainer.push_to_hub(**kwargs)\n",
      "    else:\n",
      "        trainer.create_model_card(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "print(elements[3000].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c93aed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"text\":\"def main():\\n    # See all possible arguments in src/transformers/training_args.py\\n    # or by passing the --help flag to this script.\\n    # We now keep distinct sets of args, for a cleaner separation of concerns.\\n\\n    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\\n    if len(sys.argv) == 2 and sys.argv[1].endswith(\\\".json\\\"):\\n        # If we pass only one argument to the script and it's the path to a json file,\\n        # let's parse it to get our arguments.\\n        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\\n    else:\\n        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\\n\\n    # Setup logging\\n    logging.basicConfig(\\n        format=\\\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\\\",\\n        datefmt=\\\"%m/%d/%Y %H:%M:%S\\\",\\n        handlers=[logging.StreamHandler(sys.stdout)],\\n    )\\n\\n    if training_args.should_log:\\n        # The default of training_args.log_level is passive, so we set log level at info here to have that default.\\n        transformers.utils.logging.set_verbosity_info()\\n\\n    log_level = training_args.get_process_log_level()\\n    logger.setLevel(log_level)\\n    transformers.utils.logging.set_verbosity(log_level)\\n    transformers.utils.logging.enable_default_handler()\\n    transformers.utils.logging.enable_explicit_format()\\n\\n    # Log on each process the small summary:\\n    logger.warning(\\n        f\\\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \\\"\\n        + f\\\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\\\"\\n    )\\n    logger.info(f\\\"Training/evaluation parameters {training_args}\\\")\\n\\n    # Set seed before initializing model.\\n    set_seed(training_args.seed)\\n\\n    # Initialize our dataset and prepare it for the 'image-classification' task.\\n    if data_args.dataset_name is not None:\\n        dataset = load_dataset(\\n            data_args.dataset_name,\\n            data_args.dataset_config_name,\\n            cache_dir=model_args.cache_dir,\\n            token=model_args.token,\\n            trust_remote_code=model_args.trust_remote_code,\\n        )\\n    else:\\n        data_files = {}\\n        if data_args.train_dir is not None:\\n            data_files[\\\"train\\\"] = os.path.join(data_args.train_dir, \\\"**\\\")\\n        if data_args.validation_dir is not None:\\n            data_files[\\\"validation\\\"] = os.path.join(data_args.validation_dir, \\\"**\\\")\\n        dataset = load_dataset(\\n            \\\"imagefolder\\\",\\n            data_files=data_files,\\n            cache_dir=model_args.cache_dir,\\n        )\\n\\n    dataset_column_names = dataset[\\\"train\\\"].column_names if \\\"train\\\" in dataset else dataset[\\\"validation\\\"].column_names\\n    if data_args.image_column_name not in dataset_column_names:\\n        raise ValueError(\\n            f\\\"--image_column_name {data_args.image_column_name} not found in dataset '{data_args.dataset_name}'. \\\"\\n            \\\"Make sure to set `--image_column_name` to the correct audio column - one of \\\"\\n            f\\\"{', '.join(dataset_column_names)}.\\\"\\n        )\\n    if data_args.label_column_name not in dataset_column_names:\\n        raise ValueError(\\n            f\\\"--label_column_name {data_args.label_column_name} not found in dataset '{data_args.dataset_name}'. \\\"\\n            \\\"Make sure to set `--label_column_name` to the correct text column - one of \\\"\\n            f\\\"{', '.join(dataset_column_names)}.\\\"\\n        )\\n\\n    def collate_fn(examples):\\n        pixel_values = torch.stack([example[\\\"pixel_values\\\"] for example in examples])\\n        labels = torch.tensor([example[data_args.label_column_name] for example in examples])\\n        return {\\\"pixel_values\\\": pixel_values, \\\"labels\\\": labels}\\n\\n    # If we don't have a validation split, split off a percentage of train as validation.\\n    data_args.train_val_split = None if \\\"validation\\\" in dataset else data_args.train_val_split\\n    if isinstance(data_args.train_val_split, float) and data_args.train_val_split > 0.0:\\n        split = dataset[\\\"train\\\"].train_test_split(data_args.train_val_split)\\n        dataset[\\\"train\\\"] = split[\\\"train\\\"]\\n        dataset[\\\"validation\\\"] = split[\\\"test\\\"]\\n\\n    # Prepare label mappings.\\n    # We'll include these in the model's config to get human readable labels in the Inference API.\\n    labels = dataset[\\\"train\\\"].features[data_args.label_column_name].names\\n    label2id, id2label = {}, {}\\n    for i, label in enumerate(labels):\\n        label2id[label] = str(i)\\n        id2label[str(i)] = label\\n\\n    # Load the accuracy metric from the datasets package\\n    metric = evaluate.load(\\\"accuracy\\\", cache_dir=model_args.cache_dir)\\n\\n    # Define our compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\\n    # predictions and label_ids field) and has to return a dictionary string to float.\\n    def compute_metrics(p):\\n        \\\"\\\"\\\"Computes accuracy on a batch of predictions\\\"\\\"\\\"\\n        return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\\n\\n    config = AutoConfig.from_pretrained(\\n        model_args.config_name or model_args.model_name_or_path,\\n        num_labels=len(labels),\\n        label2id=label2id,\\n        id2label=id2label,\\n        finetuning_task=\\\"image-classification\\\",\\n        cache_dir=model_args.cache_dir,\\n        revision=model_args.model_revision,\\n        token=model_args.token,\\n        trust_remote_code=model_args.trust_remote_code,\\n    )\\n    model = AutoModelForImageClassification.from_pretrained(\\n        model_args.model_name_or_path,\\n        from_tf=bool(\\\".ckpt\\\" in model_args.model_name_or_path),\\n        config=config,\\n        cache_dir=model_args.cache_dir,\\n        revision=model_args.model_revision,\\n        token=model_args.token,\\n        trust_remote_code=model_args.trust_remote_code,\\n        ignore_mismatched_sizes=model_args.ignore_mismatched_sizes,\\n    )\\n    image_processor = AutoImageProcessor.from_pretrained(\\n        model_args.image_processor_name or model_args.model_name_or_path,\\n        cache_dir=model_args.cache_dir,\\n        revision=model_args.model_revision,\\n        token=model_args.token,\\n        trust_remote_code=model_args.trust_remote_code,\\n    )\\n\\n    # Define torchvision transforms to be applied to each image.\\n    if isinstance(image_processor, TimmWrapperImageProcessor):\\n        _train_transforms = image_processor.train_transforms\\n        _val_transforms = image_processor.val_transforms\\n    else:\\n        if \\\"shortest_edge\\\" in image_processor.size:\\n            size = image_processor.size[\\\"shortest_edge\\\"]\\n        else:\\n            size = (image_processor.size[\\\"height\\\"], image_processor.size[\\\"width\\\"])\\n\\n        # Create normalization transform\\n        if hasattr(image_processor, \\\"image_mean\\\") and hasattr(image_processor, \\\"image_std\\\"):\\n            normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\\n        else:\\n            normalize = Lambda(lambda x: x)\\n        _train_transforms = Compose(\\n            [\\n                RandomResizedCrop(size),\\n                RandomHorizontalFlip(),\\n                ToTensor(),\\n                normalize,\\n            ]\\n        )\\n        _val_transforms = Compose(\\n            [\\n                Resize(size),\\n                CenterCrop(size),\\n                ToTensor(),\\n                normalize,\\n            ]\\n        )\\n\\n    def train_transforms(example_batch):\\n        \\\"\\\"\\\"Apply _train_transforms across a batch.\\\"\\\"\\\"\\n        example_batch[\\\"pixel_values\\\"] = [\\n            _train_transforms(pil_img.convert(\\\"RGB\\\")) for pil_img in example_batch[data_args.image_column_name]\\n        ]\\n        return example_batch\\n\\n    def val_transforms(example_batch):\\n        \\\"\\\"\\\"Apply _val_transforms across a batch.\\\"\\\"\\\"\\n        example_batch[\\\"pixel_values\\\"] = [\\n            _val_transforms(pil_img.convert(\\\"RGB\\\")) for pil_img in example_batch[data_args.image_column_name]\\n        ]\\n        return example_batch\\n\\n    if training_args.do_train:\\n        if \\\"train\\\" not in dataset:\\n            raise ValueError(\\\"--do_train requires a train dataset\\\")\\n        if data_args.max_train_samples is not None:\\n            dataset[\\\"train\\\"] = (\\n                dataset[\\\"train\\\"].shuffle(seed=training_args.seed).select(range(data_args.max_train_samples))\\n            )\\n        # Set the training transforms\\n        dataset[\\\"train\\\"].set_transform(train_transforms)\\n\\n    if training_args.do_eval:\\n        if \\\"validation\\\" not in dataset:\\n            raise ValueError(\\\"--do_eval requires a validation dataset\\\")\\n        if data_args.max_eval_samples is not None:\\n            dataset[\\\"validation\\\"] = (\\n                dataset[\\\"validation\\\"].shuffle(seed=training_args.seed).select(range(data_args.max_eval_samples))\\n            )\\n        # Set the validation transforms\\n        dataset[\\\"validation\\\"].set_transform(val_transforms)\\n\\n    # Initialize our trainer\\n    trainer = Trainer(\\n        model=model,\\n        args=training_args,\\n        train_dataset=dataset[\\\"train\\\"] if training_args.do_train else None,\\n        eval_dataset=dataset[\\\"validation\\\"] if training_args.do_eval else None,\\n        compute_metrics=compute_metrics,\\n        processing_class=image_processor,\\n        data_collator=collate_fn,\\n    )\\n\\n    # Training\\n    if training_args.do_train:\\n        checkpoint = None\\n        if training_args.resume_from_checkpoint is not None:\\n            checkpoint = training_args.resume_from_checkpoint\\n        train_result = trainer.train(resume_from_checkpoint=checkpoint)\\n        trainer.save_model()\\n        trainer.log_metrics(\\\"train\\\", train_result.metrics)\\n        trainer.save_metrics(\\\"train\\\", train_result.metrics)\\n        trainer.save_state()\\n\\n    # Evaluation\\n    if training_args.do_eval:\\n        metrics = trainer.evaluate()\\n        trainer.log_metrics(\\\"eval\\\", metrics)\\n        trainer.save_metrics(\\\"eval\\\", metrics)\\n\\n    # Write model card and (optionally) push to hub\\n    kwargs = {\\n        \\\"finetuned_from\\\": model_args.model_name_or_path,\\n        \\\"tasks\\\": \\\"image-classification\\\",\\n        \\\"dataset\\\": data_args.dataset_name,\\n        \\\"tags\\\": [\\\"image-classification\\\", \\\"vision\\\"],\\n    }\\n    if training_args.push_to_hub:\\n        trainer.push_to_hub(**kwargs)\\n    else:\\n        trainer.create_model_card(**kwargs)\",\"source\":\"examples/pytorch/image-classification/run_image_classification.py\",\"header\":\"import logging\\nimport os\\nimport sys\\nfrom dataclasses import dataclass, field\\nfrom typing import Optional\\nimport evaluate\\nimport numpy as np\\nimport torch\\nfrom datasets import load_dataset\\nfrom PIL import Image\\nfrom torchvision.transforms import (\\n    CenterCrop,\\n    Compose,\\n    Lambda,\\n    Normalize,\\n    RandomHorizontalFlip,\\n    RandomResizedCrop,\\n    Resize,\\n    ToTensor,\\n)\\nimport transformers\\nfrom transformers import (\\n    MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING,\\n    AutoConfig,\\n    AutoImageProcessor,\\n    AutoModelForImageClassification,\\n    HfArgumentParser,\\n    TimmWrapperImageProcessor,\\n    Trainer,\\n    TrainingArguments,\\n    set_seed,\\n)\\nfrom transformers.utils import check_min_version\\nfrom transformers.utils.versions import require_version\\n\\\"\\\"\\\" Fine-tuning a 🤗 Transformers model for image classification\\\"\\\"\\\"\\nlogger = logging.getLogger(__name__)\\ncheck_min_version(\\\"4.57.0.dev0\\\")\\nrequire_version(\\\"datasets>=2.14.0\\\", \\\"To fix: pip install -r examples/pytorch/image-classification/requirements.txt\\\")\\nMODEL_CONFIG_CLASSES = list(MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING.keys())\\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\\nif __name__ == \\\"__main__\\\":\\n    main()\",\"extension\":\".py\",\"description\":\"\"}\n"
     ]
    }
   ],
   "source": [
    "print(elements[3000].model_dump_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b023db5f-d82f-4d1d-8ed1-519808faeab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.indexing.indexer import Indexer\n",
    "\n",
    "code_element = elements[3000]\n",
    "indexer = Indexer(None, None, None, None)\n",
    "summary = await indexer.summarize_element(code_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fff570fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This comprehensive Python script facilitates fine-tuning and evaluation of image classification models using the Hugging Face Transformers library, specifically targeting tasks within the vision domain. It provides a flexible command-line interface for configuring model parameters, dataset sources, training settings, and logging preferences, making it suitable for researchers and developers working on computer vision projects. The script employs the `HfArgumentParser` to parse arguments from either command-line inputs or JSON configuration files, enabling seamless customization of model, data, and training configurations through the `ModelArguments`, `DataTrainingArguments`, and `TrainingArguments` dataclasses. It sets up detailed logging and verbosity controls to monitor training progress and system information, including process rank, device type, GPU count, and precision mode, which are critical for distributed training scenarios. The dataset loading logic supports both remote datasets via the `datasets` library and local image folders, with validation for correct column names such as `image_column_name` and `label_column_name`. Data preprocessing involves applying torchvision transforms like resizing, cropping, normalization, and augmentation, tailored to the specific image processor used, whether standard or wrapped with Timm. The script dynamically splits datasets into training and validation subsets if necessary, and constructs label mappings for human-readable output. It integrates evaluation metrics like accuracy through the `evaluate` library, defining a `compute_metrics` function for model performance assessment. Model configuration and loading leverage `AutoConfig` and `AutoModelForImageClassification`, with support for remote or local models, while image preprocessing is handled via `AutoImageProcessor`. The training pipeline is managed by the `Trainer` class, which handles training, evaluation, checkpoint resumption, and model saving, with optional push to the Hugging Face Hub. Overall, this script encapsulates a complete workflow for training, evaluating, and deploying image classification models, making it a valuable resource for vision model development and experimentation within the Hugging Face ecosystem.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
